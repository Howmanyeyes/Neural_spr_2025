{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74742617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a780aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('data/Price_euros_train.csv')\n",
    "test_set = pd.read_csv('data/Price_euros_test.csv')\n",
    "sub_set = pd.read_csv('data/Sample_submition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1005adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_null_rows(dataframe):\n",
    "    cleaned_df = dataframe.dropna()\n",
    "    print(f\"Rows removed: {len(dataframe) - len(cleaned_df)}\")\n",
    "    return cleaned_df\n",
    "def auto_fill_nulls(dataframe):\n",
    "    filled_df = dataframe.copy()\n",
    "    for column in filled_df.columns:\n",
    "        if filled_df[column].isnull().sum() > 0:  \n",
    "            if np.issubdtype(filled_df[column].dtype, np.number):\n",
    "                median_value = filled_df[column].median()\n",
    "                filled_df[column].fillna(median_value, inplace=True)\n",
    "                print(f\"Filled nulls in numeric column '{column}' with median value: {median_value}\")\n",
    "            else:\n",
    "                top_3_values = filled_df[column].value_counts().index[:3]\n",
    "                if len(top_3_values) > 0:  \n",
    "                    random_choice = np.random.choice(top_3_values)\n",
    "                    filled_df[column].fillna(random_choice, inplace=True)\n",
    "                    print(f\"Filled nulls in non-numeric column '{column}' with random choice from top 3: {random_choice}\")\n",
    "                else:\n",
    "                    print(f\"Column '{column}' has no valid non-null values to use for filling.\")\n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c890f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAM_to_int(df: pd.DataFrame):\n",
    "    if 'Ram' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df['Ram'] = (\n",
    "        df['Ram']\n",
    "        .str.replace('[A-Za-z]', '', regex=True)\n",
    "        .str.strip()\n",
    "        .astype(float)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def kg_to_float(df, column_name):\n",
    "    if 'Weight' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[column_name] = (\n",
    "        df[column_name]\n",
    "        .str.replace('kg', '', regex=False)\n",
    "        .str.strip()\n",
    "        .astype(float)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def gpu_to_int(df):\n",
    "    if 'Gpu' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    conditions = [\n",
    "        df['Gpu'].str.contains('intel', case=False, na=False),\n",
    "        df['Gpu'].str.contains('amd', case=False, na=False),\n",
    "        df['Gpu'].str.contains('nvidia', case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    choices = [1, 2, 3]\n",
    "    df['Gpu'] = np.select(conditions, choices, default=4).astype(int)\n",
    "    return df\n",
    "\n",
    "def cpu_to_freq(df):\n",
    "    if 'Cpu' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df['Cpu'] = df['Cpu'].str.extract(r'(\\d+(?:\\.\\d+)?)\\s*GHz', expand=False)\n",
    "    df['Cpu'] = pd.to_numeric(df['Cpu'], errors='coerce') \n",
    "    return df\n",
    "\n",
    "def screen_resolution_categorize(df):\n",
    "    if 'ScreenResolution' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    resolution_pattern = r'(\\d+)x(\\d+)'\n",
    "    \n",
    "    resolutions = df['ScreenResolution'].str.extract(resolution_pattern)\n",
    "    \n",
    "    resolutions = resolutions.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    df['ScreenResolution_Product'] = resolutions[0] * resolutions[1]\n",
    "    \n",
    "    df['ScreenResolution'] = df['ScreenResolution_Product'].apply(\n",
    "        lambda x: 1 if x < 2073600 else (2 if x == 2073600 else 3)\n",
    "    )\n",
    "    \n",
    "    df.drop('ScreenResolution_Product', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def memory_to_features(df):\n",
    "    if 'Memory' not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    \n",
    "    def parse_memory(mem_str):\n",
    "        parts = mem_str.split('+')\n",
    "        \n",
    "        total_hdd = 0\n",
    "        total_ssd = 0\n",
    "        total_flash = 0\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip().lower()\n",
    "            match = re.search(r'(\\d+)(tb|gb)', part)\n",
    "            if not match:\n",
    "                continue\n",
    "            size = int(match.group(1))\n",
    "            unit = match.group(2)\n",
    "            \n",
    "            if unit == 'tb':\n",
    "                size *= 1000\n",
    "            \n",
    "            if 'hdd' in part:\n",
    "                total_hdd += size\n",
    "            elif 'ssd' in part:\n",
    "                total_ssd += size\n",
    "            elif 'flash' in part:\n",
    "                total_flash += size\n",
    "        \n",
    "        return total_hdd, total_ssd, total_flash\n",
    "    \n",
    "    parsed = df['Memory'].apply(parse_memory)\n",
    "    df['Total_HDD_GB'] = parsed.apply(lambda x: x[0])\n",
    "    df['Total_SSD_GB'] = parsed.apply(lambda x: x[1])\n",
    "    df['Total_Flash_GB'] = parsed.apply(lambda x: x[2])\n",
    "    \n",
    "    df.drop('Memory', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def frequency_encode(df, column_name):\n",
    "    freq = df[column_name].value_counts()\n",
    "    df[column_name] = df[column_name].map(freq)\n",
    "    return df\n",
    "\n",
    "def frequency_encode_companies_products(df):\n",
    "    df = df.copy()\n",
    "    if 'Company' in df.columns:\n",
    "        df = frequency_encode(df, 'Company')\n",
    "    if 'Product' in df.columns:\n",
    "        df = frequency_encode(df, 'Product')\n",
    "    if 'OpSys' in df.columns:\n",
    "        df = frequency_encode(df, 'OpSys')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32f3b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('ram_to_int', FunctionTransformer(RAM_to_int, validate=False)),\n",
    "    ('weight_to_float', FunctionTransformer(lambda df: kg_to_float(df, 'Weight'), validate=False)),\n",
    "    ('gpu_to_int', FunctionTransformer(gpu_to_int, validate=False)),\n",
    "    ('cpu_to_freq', FunctionTransformer(cpu_to_freq, validate=False)),\n",
    "    ('screen_resolution', FunctionTransformer(screen_resolution_categorize, validate=False)),\n",
    "    ('memory_to_features', FunctionTransformer(memory_to_features, validate=False)),\n",
    "    ('freq_encode', FunctionTransformer(frequency_encode_companies_products, validate=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d8c8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled nulls in non-numeric column 'ScreenResolution' with random choice from top 3: IPS Panel Full HD 1920x1080\n",
      "Filled nulls in non-numeric column 'OpSys' with random choice from top 3: Linux\n",
      "Filled nulls in non-numeric column 'Weight' with random choice from top 3: 2.4kg\n",
      "Filled nulls in non-numeric column 'ScreenResolution' with random choice from top 3: 1366x768\n",
      "Filled nulls in non-numeric column 'OpSys' with random choice from top 3: Windows 7\n",
      "Filled nulls in non-numeric column 'Weight' with random choice from top 3: 2.3kg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n",
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n",
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n",
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n",
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n",
      "C:\\Users\\1TB\\AppData\\Local\\Temp\\ipykernel_16592\\618136128.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filled_df[column].fillna(random_choice, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    train_set = pd.read_csv('data/Price_euros_train.csv')\n",
    "    test_set = pd.read_csv('data/Price_euros_test.csv')\n",
    "\n",
    "    # Preprocess (fill nulls and drop irrelevant columns)\n",
    "    train_set = auto_fill_nulls(train_set).drop(columns=['laptop_ID', 'TypeName'])\n",
    "    test_set = auto_fill_nulls(test_set).drop(columns=['laptop_ID', 'TypeName'])\n",
    "\n",
    "    # Split into features and target\n",
    "    X = train_set.drop('Price_euros', axis=1)\n",
    "    y = train_set['Price_euros']\n",
    "\n",
    "    # Train-test split (on training data!)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit-transform only on training set\n",
    "    X_train = pipeline.fit_transform(X_train)\n",
    "    X_test = pipeline.transform(X_test)\n",
    "    X_val = pipeline.transform(test_set)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_val\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_val = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc4c2308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ram",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Gpu",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cpu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ScreenResolution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_HDD_GB",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_SSD_GB",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Flash_GB",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Company",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Product",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "OpSys",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sss",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "620f684d-f980-4c2c-a37f-9357dc03c827",
       "rows": [
        [
         "1001",
         "4.0",
         "2.2",
         "1",
         "1.6",
         "1",
         "500",
         "0",
         "0",
         "108",
         "1",
         "690",
         null
        ],
        [
         "1061",
         "4.0",
         "1.6",
         "1",
         "2.5",
         "1",
         "500",
         "0",
         "0",
         "193",
         "8",
         "690",
         null
        ],
        [
         "730",
         "8.0",
         "1.62",
         "1",
         "2.3",
         "2",
         "1000",
         "0",
         "0",
         "193",
         "1",
         "690",
         null
        ],
        [
         "949",
         "8.0",
         "1.16",
         "1",
         "2.5",
         "2",
         "0",
         "512",
         "0",
         "189",
         "4",
         "690",
         null
        ],
        [
         "292",
         "8.0",
         "1.48",
         "1",
         "2.6",
         "2",
         "0",
         "256",
         "0",
         "189",
         "6",
         "690",
         null
        ],
        [
         "294",
         "8.0",
         "1.84",
         "1",
         "2.5",
         "2",
         "0",
         "256",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "2",
         "8.0",
         "1.37",
         "1",
         "3.1",
         "3",
         "0",
         "256",
         "0",
         "15",
         "9",
         "8",
         null
        ],
        [
         "6",
         "4.0",
         "1.86",
         "1",
         "2.5",
         "1",
         "500",
         "0",
         "0",
         "189",
         "14",
         "45",
         null
        ],
        [
         "589",
         "4.0",
         "1.86",
         "1",
         "1.1",
         "1",
         "0",
         "128",
         "0",
         "108",
         "1",
         "690",
         null
        ],
        [
         "244",
         "4.0",
         "2.25",
         "2",
         "2.0",
         "2",
         "1000",
         "0",
         "0",
         "193",
         "17",
         "87",
         null
        ],
        [
         "861",
         "4.0",
         "1.64",
         "1",
         "2.5",
         "1",
         "500",
         "0",
         "0",
         "189",
         "6",
         "690",
         null
        ],
        [
         "962",
         "8.0",
         "1.26",
         "1",
         "2.4",
         "1",
         "0",
         "256",
         "0",
         "189",
         "7",
         "24",
         null
        ],
        [
         "1097",
         "4.0",
         "2.2",
         "1",
         "1.6",
         "1",
         "500",
         "0",
         "0",
         "108",
         "2",
         "690",
         null
        ],
        [
         "543",
         "16.0",
         "2.62",
         "3",
         "2.8",
         "2",
         "1000",
         "256",
         "0",
         "193",
         "5",
         "690",
         null
        ],
        [
         "887",
         "4.0",
         "2.04",
         "1",
         "2.5",
         "1",
         "500",
         "0",
         "0",
         "189",
         "16",
         "690",
         null
        ],
        [
         "215",
         "8.0",
         "1.37",
         "1",
         "3.1",
         "3",
         "0",
         "512",
         "0",
         "15",
         "9",
         "8",
         null
        ],
        [
         "798",
         "4.0",
         "2.18",
         "1",
         "2.0",
         "1",
         "1000",
         "0",
         "0",
         "193",
         "12",
         "690",
         null
        ],
        [
         "536",
         "4.0",
         "2.2",
         "2",
         "1.6",
         "2",
         "1000",
         "0",
         "0",
         "193",
         "14",
         "690",
         null
        ],
        [
         "1029",
         "8.0",
         "1.91",
         "1",
         "2.5",
         "1",
         "0",
         "256",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "381",
         "16.0",
         "2.5",
         "3",
         "2.9",
         "2",
         "0",
         "512",
         "0",
         "205",
         "2",
         "690",
         null
        ],
        [
         "504",
         "12.0",
         "2.2",
         "1",
         "1.8",
         "1",
         "1000",
         "0",
         "0",
         "205",
         "1",
         "690",
         null
        ],
        [
         "595",
         "4.0",
         "2.2",
         "2",
         "2.0",
         "1",
         "1000",
         "0",
         "0",
         "193",
         "17",
         "87",
         null
        ],
        [
         "554",
         "16.0",
         "1.14",
         "1",
         "2.7",
         "2",
         "0",
         "512",
         "0",
         "205",
         "9",
         "690",
         null
        ],
        [
         "660",
         "4.0",
         "2.8",
         "3",
         "1.1",
         "1",
         "1000",
         "0",
         "0",
         "108",
         "1",
         "690",
         null
        ],
        [
         "682",
         "8.0",
         "1.42",
         "1",
         "2.5",
         "3",
         "0",
         "256",
         "0",
         "205",
         "9",
         "690",
         null
        ],
        [
         "328",
         "4.0",
         "1.59",
         "1",
         "1.1",
         "1",
         "0",
         "128",
         "0",
         "205",
         "1",
         "690",
         null
        ],
        [
         "679",
         "4.0",
         "1.22",
         "1",
         "1.1",
         "2",
         "0",
         "0",
         "32",
         "4",
         "1",
         "690",
         null
        ],
        [
         "785",
         "4.0",
         "2.18",
         "2",
         "2.5",
         "1",
         "1000",
         "0",
         "0",
         "193",
         "12",
         "690",
         null
        ],
        [
         "898",
         "4.0",
         "1.2",
         "1",
         "1.6",
         "1",
         "0",
         "0",
         "16",
         "108",
         "1",
         "18",
         null
        ],
        [
         "289",
         "8.0",
         "2.0",
         "1",
         "2.5",
         "2",
         "0",
         "256",
         "0",
         "33",
         "11",
         "690",
         null
        ],
        [
         "273",
         "16.0",
         "2.8",
         "3",
         "2.8",
         "2",
         "1000",
         "256",
         "0",
         "33",
         "1",
         "690",
         null
        ],
        [
         "707",
         "32.0",
         "3.49",
         "3",
         "2.9",
         "3",
         "0",
         "512",
         "0",
         "3",
         "2",
         "690",
         null
        ],
        [
         "25",
         "2.0",
         "2.4",
         "2",
         "1.5",
         "1",
         "0",
         "0",
         "32",
         "108",
         "1",
         "690",
         null
        ],
        [
         "309",
         "16.0",
         "2.4",
         "3",
         "2.5",
         "2",
         "0",
         "512",
         "0",
         "33",
         "1",
         "690",
         null
        ],
        [
         "616",
         "6.0",
         "2.71",
         "2",
         "2.5",
         "2",
         "2000",
         "0",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "1046",
         "4.0",
         "2.18",
         "1",
         "2.5",
         "1",
         "1000",
         "0",
         "0",
         "193",
         "12",
         "87",
         null
        ],
        [
         "106",
         "6.0",
         "2.1",
         "1",
         "2.4",
         "2",
         "1000",
         "0",
         "0",
         "75",
         "1",
         "690",
         null
        ],
        [
         "55",
         "8.0",
         "1.86",
         "1",
         "2.5",
         "2",
         "0",
         "256",
         "0",
         "189",
         "14",
         "690",
         null
        ],
        [
         "1079",
         "6.0",
         "2.04",
         "2",
         "2.9",
         "2",
         "0",
         "0",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "213",
         "16.0",
         "3.6",
         "3",
         "2.9",
         "2",
         "0",
         "256",
         "0",
         "108",
         "1",
         "690",
         null
        ],
        [
         "120",
         "8.0",
         "3.0",
         "3",
         "2.5",
         "2",
         "1000",
         "128",
         "0",
         "108",
         "1",
         "690",
         null
        ],
        [
         "447",
         "8.0",
         "2.04",
         "3",
         "2.5",
         "2",
         "0",
         "256",
         "0",
         "189",
         "3",
         "690",
         null
        ],
        [
         "704",
         "4.0",
         "1.75",
         "1",
         "2.1",
         "1",
         "0",
         "128",
         "0",
         "33",
         "11",
         "690",
         null
        ],
        [
         "72",
         "4.0",
         "2.2",
         "2",
         "2.5",
         "2",
         "0",
         "128",
         "0",
         "205",
         "4",
         "690",
         null
        ],
        [
         "631",
         "4.0",
         "2.1",
         "2",
         "3.0",
         "2",
         "1000",
         "0",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "355",
         "8.0",
         "1.1",
         "1",
         "2.7",
         "2",
         "0",
         "512",
         "0",
         "108",
         "2",
         "690",
         null
        ],
        [
         "668",
         "16.0",
         "1.95",
         "3",
         "2.8",
         "2",
         "0",
         "512",
         "0",
         "3",
         "2",
         "690",
         null
        ],
        [
         "405",
         "8.0",
         "1.5",
         "2",
         "2.7",
         "2",
         "0",
         "512",
         "0",
         "205",
         "1",
         "45",
         null
        ],
        [
         "435",
         "8.0",
         "1.91",
         "2",
         "2.5",
         "1",
         "0",
         "256",
         "0",
         "189",
         "1",
         "690",
         null
        ],
        [
         "954",
         "8.0",
         "3.52",
         "3",
         "2.6",
         "2",
         "1000",
         "128",
         "0",
         "108",
         "1",
         "690",
         null
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 886
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ram</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Total_HDD_GB</th>\n",
       "      <th>Total_SSD_GB</th>\n",
       "      <th>Total_Flash_GB</th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>sss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>8</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>4</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>6</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>16</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>3</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>12</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ram  Weight  Gpu  Cpu  ScreenResolution  Total_HDD_GB  Total_SSD_GB  \\\n",
       "1001   4.0    2.20    1  1.6                 1           500             0   \n",
       "1061   4.0    1.60    1  2.5                 1           500             0   \n",
       "730    8.0    1.62    1  2.3                 2          1000             0   \n",
       "949    8.0    1.16    1  2.5                 2             0           512   \n",
       "292    8.0    1.48    1  2.6                 2             0           256   \n",
       "...    ...     ...  ...  ...               ...           ...           ...   \n",
       "466    4.0    2.10    1  2.4                 2             0           128   \n",
       "121    4.0    2.20    3  2.5                 2          1000             0   \n",
       "1044   4.0    2.65    1  1.6                 1          1000             0   \n",
       "1095  16.0    1.30    1  2.5                 3             0           512   \n",
       "860    4.0    2.18    1  2.4                 1             0           128   \n",
       "\n",
       "      Total_Flash_GB  Company  Product  OpSys  sss  \n",
       "1001               0      108        1    690  NaN  \n",
       "1061               0      193        8    690  NaN  \n",
       "730                0      193        1    690  NaN  \n",
       "949                0      189        4    690  NaN  \n",
       "292                0      189        6    690  NaN  \n",
       "...              ...      ...      ...    ...  ...  \n",
       "466                0      189       16    690  NaN  \n",
       "121                0      205        4     45  NaN  \n",
       "1044               0      108        1    690  NaN  \n",
       "1095               0      205        3    690  NaN  \n",
       "860                0      193       12    690  NaN  \n",
       "\n",
       "[886 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feature_names = [\n",
    "    'Ram', \n",
    "    'Weight', \n",
    "    'Gpu', \n",
    "    'Cpu', \n",
    "    'ScreenResolution', \n",
    "    'Total_HDD_GB', \n",
    "    'Total_SSD_GB', \n",
    "    'Total_Flash_GB', \n",
    "    'Company', \n",
    "    'Product', 'OpSys', \"sss\"\n",
    "]\n",
    "data_df = pd.DataFrame(X_train, columns=final_feature_names)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d5b3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14e78732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X =torch.from_numpy(X.to_numpy().astype(np.float32))\n",
    "        self.y =torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], self.y[index])\n",
    "     \n",
    "data = MyDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53b2a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ReLU = nn.ReLU()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.view(-1)\n",
    "    \n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "38940e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        dataset: класс, хранящий данные\n",
    "        loss_f: функция потерь\n",
    "        learning_rate: величина градиентного шага\n",
    "        epoch_amount: общее количество эпох\n",
    "        batch_size: размер одного бача\n",
    "        max_batches_per_epoch: максимальное количество бачей,\n",
    "                               подаваемых в модель в одну эпоху\n",
    "        device: устройство для вычислений\n",
    "        early_stopping: количество эпох без улучшений до остановки обучения\n",
    "        optim: оптимизатор\n",
    "        scheduler: регулятор градиентного шага\n",
    "        permutate: перемешивание тренировочной выборки перед обучением\n",
    "\n",
    "    Attributes:\n",
    "        start_model: необученная модель\n",
    "        best_model: модель, после обучения\n",
    "        train_loss: средние значения функции потерь на тренировочных\n",
    "                    данных в каждой эпохе\n",
    "        val_loss: средние значения функции потерь на валидационных\n",
    "                  данных в каждой эпохе\n",
    "\n",
    "    Methods:\n",
    "        fit: обучение модели\n",
    "        predict: возвращает предсказание обученной моделью\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # это классический конструктор класса, он учитывает много различных\n",
    "    # параметров, часть из которых задаётся по умолчанию\n",
    "    def __init__(self,  dataset, net, loss_f, learning_rate=1e-3,\n",
    "                epoch_amount=10, batch_size=12,\n",
    "                max_batches_per_epoch=None,\n",
    "                device='cpu', early_stopping=10,\n",
    "                optim=torch.optim.Adam,\n",
    "                scheduler=None, permutate=True):\n",
    "\n",
    "        self.loss_f = loss_f\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch_amount = epoch_amount\n",
    "        self.batch_size = batch_size\n",
    "        self.max_batches_per_epoch = max_batches_per_epoch\n",
    "        self.device = device\n",
    "        self.early_stopping = early_stopping\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.permutate = permutate\n",
    "        self.dataset = dataset\n",
    "        self.start_model = net\n",
    "        self.best_model = net\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    # метод используется для предсказания значения, выбирает лучшую модель\n",
    "    # из ранее обученных\n",
    "    def predict(self, X):\n",
    "        return self.best_model(X)\n",
    "\n",
    "    # большой метод для тренировки\n",
    "    # вообще говоря, его можно было разбить ещё на один метод, описывающий\n",
    "    # тренировку одно отдельной эпоих\n",
    "    def fit(self, X_train, X_test, y_train, y_test):\n",
    "\n",
    "        Net = self.start_model # берём исходную модель\n",
    "\n",
    "        device = torch.device(self.device)\n",
    "\n",
    "        Net.to(device) # переносим модель на видеокарту\n",
    "\n",
    "        # задаём оптимизатор\n",
    "        optimizer = self.optim(Net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # расписание изменения (уменьшения) скорости изменения и его шаг\n",
    "        if self.scheduler is not None:\n",
    "            scheduler = self.scheduler(optimizer)\n",
    "\n",
    "        # создаём из данных dataset и загрузчик\n",
    "        train = self.dataset(X_train, y_train)\n",
    "        val = self.dataset(X_test, y_test)\n",
    "\n",
    "        train = DataLoader(train, batch_size=self.batch_size, shuffle=self.permutate)\n",
    "        val = DataLoader(val, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        best_val_loss = float('inf') # Лучшее значение функции потерь на валидационной выборке\n",
    "                                     # функции потерь на валидационной выборке\n",
    "        best_ep = 0                  # Эпоха, на которой достигалось лучшее\n",
    "                                     # значение функции потерь на валидационной выборке\n",
    "\n",
    "        # начинаем обучение по эпохам\n",
    "        for epoch in range(self.epoch_amount):\n",
    "            start = dt.datetime.now()\n",
    "            print(f'Эпоха: {epoch}', end=' ')\n",
    "            Net.train() # переводим сеть в режим обучения\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "\n",
    "            # разбиваем данные на признаки и значения\n",
    "            for batch_X, target in train:\n",
    "                if self.max_batches_per_epoch is not None:\n",
    "                    if batch_n >= self.max_batches_per_epoch:\n",
    "                        break\n",
    "\n",
    "                # обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # переносим данные на карточку\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                # предсказываем значения\n",
    "                predicted_values = Net(batch_X)\n",
    "                loss = self.loss_f(predicted_values, target) # считаем loss\n",
    "                loss.backward() # считаем градиенты\n",
    "                optimizer.step() # меняем значения весов\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n # считаем средний loss по батчам\n",
    "            self.train_loss.append(mean_loss)\n",
    "\n",
    "            print(f'Loss_train: {mean_loss}, {dt.datetime.now() - start} сек')\n",
    "\n",
    "            Net.eval() # переводим модель в режим оценки\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_X, target in val:\n",
    "                    if self.max_batches_per_epoch is not None:\n",
    "                        if batch_n >= self.max_batches_per_epoch:\n",
    "                            break\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                predicted_values = Net(batch_X)\n",
    "                loss = self.loss_f(predicted_values, target)\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n\n",
    "            self.val_loss.append(mean_loss)\n",
    "            print(f'Loss_val: {mean_loss}')\n",
    "\n",
    "            # вводим механимз ранней остановки\n",
    "            if mean_loss < best_val_loss:\n",
    "                self.best_model = Net\n",
    "                best_val_loss = mean_loss\n",
    "                best_ep = epoch\n",
    "            elif epoch - best_ep > self.early_stopping:\n",
    "                print(f'{self.early_stopping} без улучшений. Прекращаем обучение...')\n",
    "                break\n",
    "            if self.scheduler is not None:\n",
    "                scheduler.step()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "733bf14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха: 0 Loss_train: 644798.1411528717, 0:00:00.125973 сек\n",
      "Loss_val: 2066878.875\n",
      "\n",
      "Эпоха: 1 Loss_train: 640930.5232263514, 0:00:00.092162 сек\n",
      "Loss_val: 2060991.125\n",
      "\n",
      "Эпоха: 2 Loss_train: 637535.2678420608, 0:00:00.095421 сек\n",
      "Loss_val: 2054933.375\n",
      "\n",
      "Эпоха: 3 Loss_train: 633837.3933699324, 0:00:00.092591 сек\n",
      "Loss_val: 2048706.5\n",
      "\n",
      "Эпоха: 4 Loss_train: 629641.0224873311, 0:00:00.102593 сек\n",
      "Loss_val: 2042761.875\n",
      "\n",
      "Эпоха: 5 Loss_train: 626971.4267314189, 0:00:00.085790 сек\n",
      "Loss_val: 2036662.625\n",
      "\n",
      "Эпоха: 6 Loss_train: 622082.2536951014, 0:00:00.095098 сек\n",
      "Loss_val: 2030649.5\n",
      "\n",
      "Эпоха: 7 Loss_train: 619781.5848817568, 0:00:00.090174 сек\n",
      "Loss_val: 2024747.5\n",
      "\n",
      "Эпоха: 8 Loss_train: 615083.4008657094, 0:00:00.102491 сек\n",
      "Loss_val: 2018815.875\n",
      "\n",
      "Эпоха: 9 Loss_train: 613312.9121621621, 0:00:00.096043 сек\n",
      "Loss_val: 2013190.125\n",
      "\n",
      "Эпоха: 10 Loss_train: 608663.0942778717, 0:00:00.116574 сек\n",
      "Loss_val: 2007224.375\n",
      "\n",
      "Эпоха: 11 Loss_train: 605727.3793285473, 0:00:00.099255 сек\n",
      "Loss_val: 2001598.375\n",
      "\n",
      "Эпоха: 12 Loss_train: 606043.0888935811, 0:00:00.090994 сек\n",
      "Loss_val: 1995878.625\n",
      "\n",
      "Эпоха: 13 Loss_train: 599243.417652027, 0:00:00.086130 сек\n",
      "Loss_val: 1990068.375\n",
      "\n",
      "Эпоха: 14 Loss_train: 597114.3810177365, 0:00:00.103675 сек\n",
      "Loss_val: 1984238.875\n",
      "\n",
      "Эпоха: 15 Loss_train: 596198.159839527, 0:00:00.096047 сек\n",
      "Loss_val: 1978784.5\n",
      "\n",
      "Эпоха: 16 Loss_train: 590557.8697212838, 0:00:00.091783 сек\n",
      "Loss_val: 1973260.875\n",
      "\n",
      "Эпоха: 17 Loss_train: 586901.0808699324, 0:00:00.091999 сек\n",
      "Loss_val: 1967395.625\n",
      "\n",
      "Эпоха: 18 Loss_train: 585005.6501266892, 0:00:00.074561 сек\n",
      "Loss_val: 1961943.625\n",
      "\n",
      "Эпоха: 19 Loss_train: 583080.2483108108, 0:00:00.082643 сек\n",
      "Loss_val: 1956699.0\n",
      "\n",
      "Эпоха: 20 Loss_train: 578705.6197212838, 0:00:00.103595 сек\n",
      "Loss_val: 1950920.125\n",
      "\n",
      "Эпоха: 21 Loss_train: 579568.6300675676, 0:00:00.094893 сек\n",
      "Loss_val: 1945650.375\n",
      "\n",
      "Эпоха: 22 Loss_train: 573583.2427153717, 0:00:00.088114 сек\n",
      "Loss_val: 1940104.375\n",
      "\n",
      "Эпоха: 23 Loss_train: 571727.5713682432, 0:00:00.094580 сек\n",
      "Loss_val: 1934738.125\n",
      "\n",
      "Эпоха: 24 Loss_train: 572940.4766680744, 0:00:00.092958 сек\n",
      "Loss_val: 1929511.625\n",
      "\n",
      "Эпоха: 25 Loss_train: 565796.691089527, 0:00:00.089585 сек\n",
      "Loss_val: 1924120.5\n",
      "\n",
      "Эпоха: 26 Loss_train: 563622.9033994932, 0:00:00.099870 сек\n",
      "Loss_val: 1918801.125\n",
      "\n",
      "Эпоха: 27 Loss_train: 561086.2019636824, 0:00:00.089210 сек\n",
      "Loss_val: 1913653.0\n",
      "\n",
      "Эпоха: 28 Loss_train: 558265.3584248311, 0:00:00.093345 сек\n",
      "Loss_val: 1908189.625\n",
      "\n",
      "Эпоха: 29 Loss_train: 556476.8883023649, 0:00:00.096516 сек\n",
      "Loss_val: 1903412.0\n",
      "\n",
      "Эпоха: 30 Loss_train: 553967.0723184121, 0:00:00.099760 сек\n",
      "Loss_val: 1897883.375\n",
      "\n",
      "Эпоха: 31 Loss_train: 553464.2796663851, 0:00:00.099595 сек\n",
      "Loss_val: 1892966.125\n",
      "\n",
      "Эпоха: 32 Loss_train: 549774.5211148649, 0:00:00.109060 сек\n",
      "Loss_val: 1888015.125\n",
      "\n",
      "Эпоха: 33 Loss_train: 548257.0419130068, 0:00:00.087083 сек\n",
      "Loss_val: 1882835.125\n",
      "\n",
      "Эпоха: 34 Loss_train: 545223.9054581926, 0:00:00.082596 сек\n",
      "Loss_val: 1877701.5\n",
      "\n",
      "Эпоха: 35 Loss_train: 543288.898754223, 0:00:00.087982 сек\n",
      "Loss_val: 1872807.0\n",
      "\n",
      "Эпоха: 36 Loss_train: 541704.6577280406, 0:00:00.101747 сек\n",
      "Loss_val: 1867423.0\n",
      "\n",
      "Эпоха: 37 Loss_train: 538574.9038217906, 0:00:00.126302 сек\n",
      "Loss_val: 1862869.375\n",
      "\n",
      "Эпоха: 38 Loss_train: 536990.1139146959, 0:00:00.105002 сек\n",
      "Loss_val: 1857788.875\n",
      "\n",
      "Эпоха: 39 Loss_train: 535263.9180743244, 0:00:00.101036 сек\n",
      "Loss_val: 1853148.625\n",
      "\n",
      "Эпоха: 40 Loss_train: 533358.1922508446, 0:00:00.086638 сек\n",
      "Loss_val: 1848393.625\n",
      "\n",
      "Эпоха: 41 Loss_train: 531964.2395481418, 0:00:00.079998 сек\n",
      "Loss_val: 1843331.375\n",
      "\n",
      "Эпоха: 42 Loss_train: 529364.1521326014, 0:00:00.095094 сек\n",
      "Loss_val: 1838546.375\n",
      "\n",
      "Эпоха: 43 Loss_train: 527503.5260768582, 0:00:00.093668 сек\n",
      "Loss_val: 1833743.625\n",
      "\n",
      "Эпоха: 44 Loss_train: 524702.4205553209, 0:00:00.099011 сек\n",
      "Loss_val: 1828967.625\n",
      "\n",
      "Эпоха: 45 Loss_train: 523202.5389569257, 0:00:00.115548 сек\n",
      "Loss_val: 1824117.0\n",
      "\n",
      "Эпоха: 46 Loss_train: 521141.2728040541, 0:00:00.084577 сек\n",
      "Loss_val: 1819682.125\n",
      "\n",
      "Эпоха: 47 Loss_train: 522146.7311021959, 0:00:00.091240 сек\n",
      "Loss_val: 1815125.375\n",
      "\n",
      "Эпоха: 48 Loss_train: 518278.79444679053, 0:00:00.094588 сек\n",
      "Loss_val: 1810396.625\n",
      "\n",
      "Эпоха: 49 Loss_train: 519126.5334670608, 0:00:00.105633 сек\n",
      "Loss_val: 1805803.125\n",
      "\n",
      "Эпоха: 50 Loss_train: 514522.43116554053, 0:00:00.087319 сек\n",
      "Loss_val: 1800989.875\n",
      "\n",
      "Эпоха: 51 Loss_train: 513000.4472128378, 0:00:00.098965 сек\n",
      "Loss_val: 1796359.5\n",
      "\n",
      "Эпоха: 52 Loss_train: 515413.66374577704, 0:00:00.094664 сек\n",
      "Loss_val: 1791962.0\n",
      "\n",
      "Эпоха: 53 Loss_train: 509306.59607263515, 0:00:00.088584 сек\n",
      "Loss_val: 1787047.625\n",
      "\n",
      "Эпоха: 54 Loss_train: 507216.8542018581, 0:00:00.102097 сек\n",
      "Loss_val: 1782543.5\n",
      "\n",
      "Эпоха: 55 Loss_train: 507077.0971283784, 0:00:00.097558 сек\n",
      "Loss_val: 1778239.5\n",
      "\n",
      "Эпоха: 56 Loss_train: 503862.2702702703, 0:00:00.100882 сек\n",
      "Loss_val: 1773429.625\n",
      "\n",
      "Эпоха: 57 Loss_train: 502539.5423880912, 0:00:00.099541 сек\n",
      "Loss_val: 1769197.875\n",
      "\n",
      "Эпоха: 58 Loss_train: 501786.97371199325, 0:00:00.084738 сек\n",
      "Loss_val: 1764609.125\n",
      "\n",
      "Эпоха: 59 Loss_train: 499424.2948690878, 0:00:00.088239 сек\n",
      "Loss_val: 1760072.125\n",
      "\n",
      "Эпоха: 60 Loss_train: 498259.63429054053, 0:00:00.096441 сек\n",
      "Loss_val: 1755918.5\n",
      "\n",
      "Эпоха: 61 Loss_train: 496626.03547297296, 0:00:00.104089 сек\n",
      "Loss_val: 1751738.625\n",
      "\n",
      "Эпоха: 62 Loss_train: 494909.59617820947, 0:00:00.095959 сек\n",
      "Loss_val: 1747127.625\n",
      "\n",
      "Эпоха: 63 Loss_train: 493050.6274809966, 0:00:00.089053 сек\n",
      "Loss_val: 1742717.625\n",
      "\n",
      "Эпоха: 64 Loss_train: 492326.94372888515, 0:00:00.097985 сек\n",
      "Loss_val: 1738513.375\n",
      "\n",
      "Эпоха: 65 Loss_train: 490866.8807010135, 0:00:00.092042 сек\n",
      "Loss_val: 1734114.375\n",
      "\n",
      "Эпоха: 66 Loss_train: 489629.0286106419, 0:00:00.102445 сек\n",
      "Loss_val: 1729747.875\n",
      "\n",
      "Эпоха: 67 Loss_train: 487010.4465793919, 0:00:00.093202 сек\n",
      "Loss_val: 1725382.625\n",
      "\n",
      "Эпоха: 68 Loss_train: 485747.3102829392, 0:00:00.097137 сек\n",
      "Loss_val: 1720972.875\n",
      "\n",
      "Эпоха: 69 Loss_train: 484339.9704391892, 0:00:00.102024 сек\n",
      "Loss_val: 1716730.125\n",
      "\n",
      "Эпоха: 70 Loss_train: 483084.83952702704, 0:00:00.084614 сек\n",
      "Loss_val: 1712490.5\n",
      "\n",
      "Эпоха: 71 Loss_train: 482403.21125422296, 0:00:00.085000 сек\n",
      "Loss_val: 1708146.0\n",
      "\n",
      "Эпоха: 72 Loss_train: 480191.8805954392, 0:00:00.094592 сек\n",
      "Loss_val: 1703906.125\n",
      "\n",
      "Эпоха: 73 Loss_train: 478635.71843327704, 0:00:00.087572 сек\n",
      "Loss_val: 1699683.625\n",
      "\n",
      "Эпоха: 74 Loss_train: 477656.7779771959, 0:00:00.100747 сек\n",
      "Loss_val: 1695522.0\n",
      "\n",
      "Эпоха: 75 Loss_train: 476101.72635135136, 0:00:00.084126 сек\n",
      "Loss_val: 1690968.875\n",
      "\n",
      "Эпоха: 76 Loss_train: 475120.11169763515, 0:00:00.094247 сек\n",
      "Loss_val: 1686790.625\n",
      "\n",
      "Эпоха: 77 Loss_train: 473708.9205025338, 0:00:00.092053 сек\n",
      "Loss_val: 1682716.375\n",
      "\n",
      "Эпоха: 78 Loss_train: 472360.49577702704, 0:00:00.086519 сек\n",
      "Loss_val: 1678845.125\n",
      "\n",
      "Эпоха: 79 Loss_train: 471522.0859375, 0:00:00.081591 сек\n",
      "Loss_val: 1674210.875\n",
      "\n",
      "Эпоха: 80 Loss_train: 469451.47761824325, 0:00:00.094027 сек\n",
      "Loss_val: 1670007.5\n",
      "\n",
      "Эпоха: 81 Loss_train: 468107.3983319257, 0:00:00.091802 сек\n",
      "Loss_val: 1666220.0\n",
      "\n",
      "Эпоха: 82 Loss_train: 467175.55891047296, 0:00:00.088104 сек\n",
      "Loss_val: 1661730.875\n",
      "\n",
      "Эпоха: 83 Loss_train: 466829.0769636824, 0:00:00.085967 сек\n",
      "Loss_val: 1657837.625\n",
      "\n",
      "Эпоха: 84 Loss_train: 464488.59670608107, 0:00:00.087555 сек\n",
      "Loss_val: 1653689.625\n",
      "\n",
      "Эпоха: 85 Loss_train: 463059.5716849662, 0:00:00.092231 сек\n",
      "Loss_val: 1649427.875\n",
      "\n",
      "Эпоха: 86 Loss_train: 462089.41775760136, 0:00:00.085423 сек\n",
      "Loss_val: 1645409.125\n",
      "\n",
      "Эпоха: 87 Loss_train: 460560.87595016893, 0:00:00.082715 сек\n",
      "Loss_val: 1641095.5\n",
      "\n",
      "Эпоха: 88 Loss_train: 459731.40783361485, 0:00:00.089125 сек\n",
      "Loss_val: 1637022.375\n",
      "\n",
      "Эпоха: 89 Loss_train: 458460.9321684966, 0:00:00.094591 сек\n",
      "Loss_val: 1632980.625\n",
      "\n",
      "Эпоха: 90 Loss_train: 457555.6563555743, 0:00:00.096912 сек\n",
      "Loss_val: 1628940.875\n",
      "\n",
      "Эпоха: 91 Loss_train: 456384.2325802365, 0:00:00.099076 сек\n",
      "Loss_val: 1625152.5\n",
      "\n",
      "Эпоха: 92 Loss_train: 455009.5256545608, 0:00:00.106791 сек\n",
      "Loss_val: 1621008.375\n",
      "\n",
      "Эпоха: 93 Loss_train: 453631.4129011824, 0:00:00.084641 сек\n",
      "Loss_val: 1616975.0\n",
      "\n",
      "Эпоха: 94 Loss_train: 456527.8743665541, 0:00:00.083890 сек\n",
      "Loss_val: 1612752.625\n",
      "\n",
      "Эпоха: 95 Loss_train: 451397.5490920608, 0:00:00.090198 сек\n",
      "Loss_val: 1608778.875\n",
      "\n",
      "Эпоха: 96 Loss_train: 450468.0709987331, 0:00:00.081556 сек\n",
      "Loss_val: 1604928.875\n",
      "\n",
      "Эпоха: 97 Loss_train: 450115.4691722973, 0:00:00.083127 сек\n",
      "Loss_val: 1600296.0\n",
      "\n",
      "Эпоха: 98 Loss_train: 447923.22297297296, 0:00:00.085215 сек\n",
      "Loss_val: 1596749.875\n",
      "\n",
      "Эпоха: 99 Loss_train: 447594.66554054053, 0:00:00.082165 сек\n",
      "Loss_val: 1592473.375\n",
      "\n",
      "Эпоха: 100 Loss_train: 445662.2780827703, 0:00:00.091182 сек\n",
      "Loss_val: 1588466.875\n",
      "\n",
      "Эпоха: 101 Loss_train: 445882.59786739864, 0:00:00.102222 сек\n",
      "Loss_val: 1584886.375\n",
      "\n",
      "Эпоха: 102 Loss_train: 443803.54054054053, 0:00:00.096200 сек\n",
      "Loss_val: 1580799.625\n",
      "\n",
      "Эпоха: 103 Loss_train: 442568.76905616553, 0:00:00.098016 сек\n",
      "Loss_val: 1576906.875\n",
      "\n",
      "Эпоха: 104 Loss_train: 442013.6935177365, 0:00:00.094000 сек\n",
      "Loss_val: 1572932.875\n",
      "\n",
      "Эпоха: 105 Loss_train: 442927.81608952704, 0:00:00.088834 сек\n",
      "Loss_val: 1569101.625\n",
      "\n",
      "Эпоха: 106 Loss_train: 440503.12214949325, 0:00:00.091695 сек\n",
      "Loss_val: 1565186.5\n",
      "\n",
      "Эпоха: 107 Loss_train: 439005.9861697635, 0:00:00.094704 сек\n",
      "Loss_val: 1561289.0\n",
      "\n",
      "Эпоха: 108 Loss_train: 437529.54919763515, 0:00:00.099370 сек\n",
      "Loss_val: 1557192.875\n",
      "\n",
      "Эпоха: 109 Loss_train: 436741.49324324325, 0:00:00.089809 сек\n",
      "Loss_val: 1553604.0\n",
      "\n",
      "Эпоха: 110 Loss_train: 435199.8157728041, 0:00:00.081572 сек\n",
      "Loss_val: 1549523.0\n",
      "\n",
      "Эпоха: 111 Loss_train: 434215.53029983107, 0:00:00.092216 сек\n",
      "Loss_val: 1545622.5\n",
      "\n",
      "Эпоха: 112 Loss_train: 433356.5013724662, 0:00:00.091135 сек\n",
      "Loss_val: 1541757.125\n",
      "\n",
      "Эпоха: 113 Loss_train: 432422.90994510136, 0:00:00.083739 сек\n",
      "Loss_val: 1538102.125\n",
      "\n",
      "Эпоха: 114 Loss_train: 431856.97687922296, 0:00:00.093035 сек\n",
      "Loss_val: 1534408.875\n",
      "\n",
      "Эпоха: 115 Loss_train: 430253.8808065878, 0:00:00.089059 сек\n",
      "Loss_val: 1530342.625\n",
      "\n",
      "Эпоха: 116 Loss_train: 429414.50680954393, 0:00:00.094515 сек\n",
      "Loss_val: 1526568.5\n",
      "\n",
      "Эпоха: 117 Loss_train: 429399.6476984797, 0:00:00.098278 сек\n",
      "Loss_val: 1523096.0\n",
      "\n",
      "Эпоха: 118 Loss_train: 427613.8168285473, 0:00:00.092674 сек\n",
      "Loss_val: 1519357.0\n",
      "\n",
      "Эпоха: 119 Loss_train: 426698.7938133446, 0:00:00.093000 сек\n",
      "Loss_val: 1515254.375\n",
      "\n",
      "Эпоха: 120 Loss_train: 426242.5070734797, 0:00:00.098629 сек\n",
      "Loss_val: 1511843.625\n",
      "\n",
      "Эпоха: 121 Loss_train: 424677.2924408784, 0:00:00.092001 сек\n",
      "Loss_val: 1507957.375\n",
      "\n",
      "Эпоха: 122 Loss_train: 423904.40983952704, 0:00:00.099584 сек\n",
      "Loss_val: 1504039.0\n",
      "\n",
      "Эпоха: 123 Loss_train: 423252.37531672296, 0:00:00.091171 сек\n",
      "Loss_val: 1500486.0\n",
      "\n",
      "Эпоха: 124 Loss_train: 421646.2540118243, 0:00:00.096116 сек\n",
      "Loss_val: 1496605.625\n",
      "\n",
      "Эпоха: 125 Loss_train: 420862.1288006757, 0:00:00.097208 сек\n",
      "Loss_val: 1492850.5\n",
      "\n",
      "Эпоха: 126 Loss_train: 420622.8168285473, 0:00:00.102664 сек\n",
      "Loss_val: 1489324.625\n",
      "\n",
      "Эпоха: 127 Loss_train: 419339.2860008446, 0:00:00.085100 сек\n",
      "Loss_val: 1485749.0\n",
      "\n",
      "Эпоха: 128 Loss_train: 418367.7409206081, 0:00:00.088122 сек\n",
      "Loss_val: 1481754.625\n",
      "\n",
      "Эпоха: 129 Loss_train: 417936.1009290541, 0:00:00.096527 сек\n",
      "Loss_val: 1478185.5\n",
      "\n",
      "Эпоха: 130 Loss_train: 417155.52502111485, 0:00:00.083535 сек\n",
      "Loss_val: 1474551.0\n",
      "\n",
      "Эпоха: 131 Loss_train: 415609.5277660473, 0:00:00.093028 сек\n",
      "Loss_val: 1470576.5\n",
      "\n",
      "Эпоха: 132 Loss_train: 415400.2191722973, 0:00:00.085061 сек\n",
      "Loss_val: 1467365.0\n",
      "\n",
      "Эпоха: 133 Loss_train: 413790.9305320946, 0:00:00.100144 сек\n",
      "Loss_val: 1463678.875\n",
      "\n",
      "Эпоха: 134 Loss_train: 413070.6627956081, 0:00:00.090675 сек\n",
      "Loss_val: 1460186.5\n",
      "\n",
      "Эпоха: 135 Loss_train: 412770.8553631757, 0:00:00.083551 сек\n",
      "Loss_val: 1456147.375\n",
      "\n",
      "Эпоха: 136 Loss_train: 411548.70597550675, 0:00:00.090669 сек\n",
      "Loss_val: 1452427.875\n",
      "\n",
      "Эпоха: 137 Loss_train: 410397.97761824325, 0:00:00.092600 сек\n",
      "Loss_val: 1449182.375\n",
      "\n",
      "Эпоха: 138 Loss_train: 409853.98469172296, 0:00:00.085118 сек\n",
      "Loss_val: 1445476.375\n",
      "\n",
      "Эпоха: 139 Loss_train: 411083.30310388515, 0:00:00.080367 сек\n",
      "Loss_val: 1442176.625\n",
      "\n",
      "Эпоха: 140 Loss_train: 408556.0864653716, 0:00:00.091030 сек\n",
      "Loss_val: 1438078.375\n",
      "\n",
      "Эпоха: 141 Loss_train: 408255.0363175676, 0:00:00.084546 сек\n",
      "Loss_val: 1434716.625\n",
      "\n",
      "Эпоха: 142 Loss_train: 406370.9044552365, 0:00:00.093015 сек\n",
      "Loss_val: 1430893.875\n",
      "\n",
      "Эпоха: 143 Loss_train: 405754.4660050676, 0:00:00.086177 сек\n",
      "Loss_val: 1427564.125\n",
      "\n",
      "Эпоха: 144 Loss_train: 404703.9370777027, 0:00:00.093144 сек\n",
      "Loss_val: 1424169.5\n",
      "\n",
      "Эпоха: 145 Loss_train: 404440.49757179053, 0:00:00.097083 сек\n",
      "Loss_val: 1420910.625\n",
      "\n",
      "Эпоха: 146 Loss_train: 403131.96774704393, 0:00:00.090642 сек\n",
      "Loss_val: 1417259.0\n",
      "\n",
      "Эпоха: 147 Loss_train: 405778.79317989864, 0:00:00.086134 сек\n",
      "Loss_val: 1414022.5\n",
      "\n",
      "Эпоха: 148 Loss_train: 402151.1061021959, 0:00:00.088018 сек\n",
      "Loss_val: 1410571.375\n",
      "\n",
      "Эпоха: 149 Loss_train: 401317.72561233107, 0:00:00.097355 сек\n",
      "Loss_val: 1406940.5\n",
      "\n",
      "Эпоха: 150 Loss_train: 400127.4902871622, 0:00:00.099658 сек\n",
      "Loss_val: 1403287.0\n",
      "\n",
      "Эпоха: 151 Loss_train: 399485.80078125, 0:00:00.098116 сек\n",
      "Loss_val: 1399964.625\n",
      "\n",
      "Эпоха: 152 Loss_train: 398682.2488386824, 0:00:00.105684 сек\n",
      "Loss_val: 1396289.5\n",
      "\n",
      "Эпоха: 153 Loss_train: 397729.0377956081, 0:00:00.097083 сек\n",
      "Loss_val: 1393195.0\n",
      "\n",
      "Эпоха: 154 Loss_train: 397158.09923986485, 0:00:00.090426 сек\n",
      "Loss_val: 1389466.875\n",
      "\n",
      "Эпоха: 155 Loss_train: 396572.93781672296, 0:00:00.095351 сек\n",
      "Loss_val: 1385979.25\n",
      "\n",
      "Эпоха: 156 Loss_train: 396210.0787584459, 0:00:00.088948 сек\n",
      "Loss_val: 1382736.0\n",
      "\n",
      "Эпоха: 157 Loss_train: 395964.0703125, 0:00:00.086806 сек\n",
      "Loss_val: 1379034.375\n",
      "\n",
      "Эпоха: 158 Loss_train: 393975.4764569257, 0:00:00.083163 сек\n",
      "Loss_val: 1375719.875\n",
      "\n",
      "Эпоха: 159 Loss_train: 393292.64432010136, 0:00:00.094447 сек\n",
      "Loss_val: 1372295.375\n",
      "\n",
      "Эпоха: 160 Loss_train: 392845.54265202704, 0:00:00.082240 сек\n",
      "Loss_val: 1369067.875\n",
      "\n",
      "Эпоха: 161 Loss_train: 392042.4948268581, 0:00:00.098962 сек\n",
      "Loss_val: 1365678.375\n",
      "\n",
      "Эпоха: 162 Loss_train: 391495.93602195947, 0:00:00.096890 сек\n",
      "Loss_val: 1362297.875\n",
      "\n",
      "Эпоха: 163 Loss_train: 390204.3468116554, 0:00:00.100083 сек\n",
      "Loss_val: 1359014.875\n",
      "\n",
      "Эпоха: 164 Loss_train: 390042.6675464527, 0:00:00.105157 сек\n",
      "Loss_val: 1355449.875\n",
      "\n",
      "Эпоха: 165 Loss_train: 389669.1680743243, 0:00:00.101788 сек\n",
      "Loss_val: 1352234.875\n",
      "\n",
      "Эпоха: 166 Loss_train: 388248.1372466216, 0:00:00.089982 сек\n",
      "Loss_val: 1349197.5\n",
      "\n",
      "Эпоха: 167 Loss_train: 387547.40519425675, 0:00:00.089169 сек\n",
      "Loss_val: 1345398.875\n",
      "\n",
      "Эпоха: 168 Loss_train: 386673.3169341216, 0:00:00.094663 сек\n",
      "Loss_val: 1342107.375\n",
      "\n",
      "Эпоха: 169 Loss_train: 386087.60842483107, 0:00:00.095577 сек\n",
      "Loss_val: 1339063.0\n",
      "\n",
      "Эпоха: 170 Loss_train: 385234.84375, 0:00:00.104823 сек\n",
      "Loss_val: 1335720.375\n",
      "\n",
      "Эпоха: 171 Loss_train: 384557.49699113175, 0:00:00.097571 сек\n",
      "Loss_val: 1332231.375\n",
      "\n",
      "Эпоха: 172 Loss_train: 384583.0559543919, 0:00:00.089534 сек\n",
      "Loss_val: 1329299.375\n",
      "\n",
      "Эпоха: 173 Loss_train: 386706.4136402027, 0:00:00.109126 сек\n",
      "Loss_val: 1325911.625\n",
      "\n",
      "Эпоха: 174 Loss_train: 384064.37109375, 0:00:00.090582 сек\n",
      "Loss_val: 1322784.5\n",
      "\n",
      "Эпоха: 175 Loss_train: 382888.39210304053, 0:00:00.093040 сек\n",
      "Loss_val: 1319223.0\n",
      "\n",
      "Эпоха: 176 Loss_train: 381814.4315878378, 0:00:00.097073 сек\n",
      "Loss_val: 1316093.125\n",
      "\n",
      "Эпоха: 177 Loss_train: 380573.6713471284, 0:00:00.096308 сек\n",
      "Loss_val: 1312633.875\n",
      "\n",
      "Эпоха: 178 Loss_train: 379990.1941511824, 0:00:00.086199 сек\n",
      "Loss_val: 1309497.625\n",
      "\n",
      "Эпоха: 179 Loss_train: 379726.2498944257, 0:00:00.097080 сек\n",
      "Loss_val: 1306420.875\n",
      "\n",
      "Эпоха: 180 Loss_train: 378826.7977195946, 0:00:00.090425 сек\n",
      "Loss_val: 1303195.625\n",
      "\n",
      "Эпоха: 181 Loss_train: 378479.2808277027, 0:00:00.083354 сек\n",
      "Loss_val: 1299960.25\n",
      "\n",
      "Эпоха: 182 Loss_train: 377183.8926309122, 0:00:00.096595 сек\n",
      "Loss_val: 1296770.75\n",
      "\n",
      "Эпоха: 183 Loss_train: 377573.95016891893, 0:00:00.086115 сек\n",
      "Loss_val: 1293678.25\n",
      "\n",
      "Эпоха: 184 Loss_train: 376034.98015202704, 0:00:00.089543 сек\n",
      "Loss_val: 1290274.625\n",
      "\n",
      "Эпоха: 185 Loss_train: 376695.3981735642, 0:00:00.102083 сек\n",
      "Loss_val: 1287250.875\n",
      "\n",
      "Эпоха: 186 Loss_train: 374507.3814400338, 0:00:00.101068 сек\n",
      "Loss_val: 1283791.75\n",
      "\n",
      "Эпоха: 187 Loss_train: 374042.26203547296, 0:00:00.095192 сек\n",
      "Loss_val: 1280600.625\n",
      "\n",
      "Эпоха: 188 Loss_train: 376896.9934543919, 0:00:00.096211 сек\n",
      "Loss_val: 1277630.125\n",
      "\n",
      "Эпоха: 189 Loss_train: 373382.60187922296, 0:00:00.088629 сек\n",
      "Loss_val: 1274574.125\n",
      "\n",
      "Эпоха: 190 Loss_train: 372512.06619510136, 0:00:00.081094 сек\n",
      "Loss_val: 1271455.375\n",
      "\n",
      "Эпоха: 191 Loss_train: 371258.1913006757, 0:00:00.093652 сек\n",
      "Loss_val: 1268240.625\n",
      "\n",
      "Эпоха: 192 Loss_train: 370917.3194679054, 0:00:00.095573 сек\n",
      "Loss_val: 1265059.125\n",
      "\n",
      "Эпоха: 193 Loss_train: 369994.2558065878, 0:00:00.101575 сек\n",
      "Loss_val: 1262042.125\n",
      "\n",
      "Эпоха: 194 Loss_train: 369633.9454180743, 0:00:00.087722 сек\n",
      "Loss_val: 1258488.875\n",
      "\n",
      "Эпоха: 195 Loss_train: 369564.1563555743, 0:00:00.095583 сек\n",
      "Loss_val: 1255790.625\n",
      "\n",
      "Эпоха: 196 Loss_train: 368469.8241131757, 0:00:00.090092 сек\n",
      "Loss_val: 1252789.25\n",
      "\n",
      "Эпоха: 197 Loss_train: 367529.078125, 0:00:00.099114 сек\n",
      "Loss_val: 1249947.125\n",
      "\n",
      "Эпоха: 198 Loss_train: 367010.3737331081, 0:00:00.093109 сек\n",
      "Loss_val: 1246547.625\n",
      "\n",
      "Эпоха: 199 Loss_train: 367005.65139358107, 0:00:00.090042 сек\n",
      "Loss_val: 1243815.75\n",
      "\n",
      "Эпоха: 200 Loss_train: 367292.5446579392, 0:00:00.091735 сек\n",
      "Loss_val: 1240814.125\n",
      "\n",
      "Эпоха: 201 Loss_train: 365000.91258445947, 0:00:00.098944 сек\n",
      "Loss_val: 1237505.375\n",
      "\n",
      "Эпоха: 202 Loss_train: 364523.8637035473, 0:00:00.093114 сек\n",
      "Loss_val: 1234915.875\n",
      "\n",
      "Эпоха: 203 Loss_train: 364563.7820945946, 0:00:00.090654 сек\n",
      "Loss_val: 1231471.0\n",
      "\n",
      "Эпоха: 204 Loss_train: 363692.39083614864, 0:00:00.098845 сек\n",
      "Loss_val: 1228568.625\n",
      "\n",
      "Эпоха: 205 Loss_train: 362597.7162162162, 0:00:00.087067 сек\n",
      "Loss_val: 1225604.375\n",
      "\n",
      "Эпоха: 206 Loss_train: 362534.0122466216, 0:00:00.093034 сек\n",
      "Loss_val: 1222582.25\n",
      "\n",
      "Эпоха: 207 Loss_train: 361342.1830658784, 0:00:00.109252 сек\n",
      "Loss_val: 1219532.75\n",
      "\n",
      "Эпоха: 208 Loss_train: 360657.79064611485, 0:00:00.086000 сек\n",
      "Loss_val: 1216564.375\n",
      "\n",
      "Эпоха: 209 Loss_train: 361213.8249577703, 0:00:00.098001 сек\n",
      "Loss_val: 1214187.625\n",
      "\n",
      "Эпоха: 210 Loss_train: 360390.3861380912, 0:00:00.094369 сек\n",
      "Loss_val: 1210760.375\n",
      "\n",
      "Эпоха: 211 Loss_train: 361167.2485747466, 0:00:00.079304 сек\n",
      "Loss_val: 1207934.125\n",
      "\n",
      "Эпоха: 212 Loss_train: 358642.6515519426, 0:00:00.095192 сек\n",
      "Loss_val: 1204676.0\n",
      "\n",
      "Эпоха: 213 Loss_train: 357707.78679265204, 0:00:00.100191 сек\n",
      "Loss_val: 1201892.375\n",
      "\n",
      "Эпоха: 214 Loss_train: 357236.4682221284, 0:00:00.090041 сек\n",
      "Loss_val: 1199006.0\n",
      "\n",
      "Эпоха: 215 Loss_train: 360152.3776393581, 0:00:00.105063 сек\n",
      "Loss_val: 1195983.625\n",
      "\n",
      "Эпоха: 216 Loss_train: 359617.2895903716, 0:00:00.101610 сек\n",
      "Loss_val: 1192835.5\n",
      "\n",
      "Эпоха: 217 Loss_train: 357217.2000633446, 0:00:00.099651 сек\n",
      "Loss_val: 1190174.125\n",
      "\n",
      "Эпоха: 218 Loss_train: 355611.8853462838, 0:00:00.087630 сек\n",
      "Loss_val: 1187181.875\n",
      "\n",
      "Эпоха: 219 Loss_train: 355354.4649493243, 0:00:00.096654 сек\n",
      "Loss_val: 1184195.25\n",
      "\n",
      "Эпоха: 220 Loss_train: 354717.47951858107, 0:00:00.095596 сек\n",
      "Loss_val: 1181704.375\n",
      "\n",
      "Эпоха: 221 Loss_train: 353727.18992820947, 0:00:00.086090 сек\n",
      "Loss_val: 1178462.5\n",
      "\n",
      "Эпоха: 222 Loss_train: 352923.5350506757, 0:00:00.087524 сек\n",
      "Loss_val: 1175553.5\n",
      "\n",
      "Эпоха: 223 Loss_train: 352609.2266680743, 0:00:00.089083 сек\n",
      "Loss_val: 1172627.125\n",
      "\n",
      "Эпоха: 224 Loss_train: 351664.9474239865, 0:00:00.098481 сек\n",
      "Loss_val: 1169867.0\n",
      "\n",
      "Эпоха: 225 Loss_train: 351726.34670608107, 0:00:00.097211 сек\n",
      "Loss_val: 1167353.625\n",
      "\n",
      "Эпоха: 226 Loss_train: 350507.24313766893, 0:00:00.096824 сек\n",
      "Loss_val: 1164370.25\n",
      "\n",
      "Эпоха: 227 Loss_train: 351843.3301836993, 0:00:00.091588 сек\n",
      "Loss_val: 1161938.125\n",
      "\n",
      "Эпоха: 228 Loss_train: 349856.2377005912, 0:00:00.106595 сек\n",
      "Loss_val: 1158707.75\n",
      "\n",
      "Эпоха: 229 Loss_train: 348965.6661739865, 0:00:00.107656 сек\n",
      "Loss_val: 1156196.875\n",
      "\n",
      "Эпоха: 230 Loss_train: 349056.9256756757, 0:00:00.109587 сек\n",
      "Loss_val: 1153257.875\n",
      "\n",
      "Эпоха: 231 Loss_train: 347928.59533361485, 0:00:00.096596 сек\n",
      "Loss_val: 1150599.375\n",
      "\n",
      "Эпоха: 232 Loss_train: 347477.26140202704, 0:00:00.096001 сек\n",
      "Loss_val: 1147826.0\n",
      "\n",
      "Эпоха: 233 Loss_train: 347196.56925675675, 0:00:00.124081 сек\n",
      "Loss_val: 1144894.625\n",
      "\n",
      "Эпоха: 234 Loss_train: 346412.0677259291, 0:00:00.087370 сек\n",
      "Loss_val: 1142088.25\n",
      "\n",
      "Эпоха: 235 Loss_train: 345706.4310071791, 0:00:00.094514 сек\n",
      "Loss_val: 1139259.625\n",
      "\n",
      "Эпоха: 236 Loss_train: 345232.18201013515, 0:00:00.099100 сек\n",
      "Loss_val: 1136494.625\n",
      "\n",
      "Эпоха: 237 Loss_train: 344612.32907516893, 0:00:00.101660 сек\n",
      "Loss_val: 1133973.5\n",
      "\n",
      "Эпоха: 238 Loss_train: 343791.8198902027, 0:00:00.095586 сек\n",
      "Loss_val: 1130866.0\n",
      "\n",
      "Эпоха: 239 Loss_train: 343795.34279983107, 0:00:00.098042 сек\n",
      "Loss_val: 1127982.625\n",
      "\n",
      "Эпоха: 240 Loss_train: 342923.5134079392, 0:00:00.086048 сек\n",
      "Loss_val: 1125538.875\n",
      "\n",
      "Эпоха: 241 Loss_train: 342707.4954603041, 0:00:00.096954 сек\n",
      "Loss_val: 1122752.625\n",
      "\n",
      "Эпоха: 242 Loss_train: 341943.8796980574, 0:00:00.095149 сек\n",
      "Loss_val: 1119877.0\n",
      "\n",
      "Эпоха: 243 Loss_train: 341418.3794341216, 0:00:00.084796 сек\n",
      "Loss_val: 1117636.875\n",
      "\n",
      "Эпоха: 244 Loss_train: 342340.6817461993, 0:00:00.097145 сек\n",
      "Loss_val: 1114800.875\n",
      "\n",
      "Эпоха: 245 Loss_train: 340476.66311233107, 0:00:00.103122 сек\n",
      "Loss_val: 1112005.5\n",
      "\n",
      "Эпоха: 246 Loss_train: 339891.5832981419, 0:00:00.098302 сек\n",
      "Loss_val: 1109170.125\n",
      "\n",
      "Эпоха: 247 Loss_train: 339296.2391258446, 0:00:00.105594 сек\n",
      "Loss_val: 1106872.625\n",
      "\n",
      "Эпоха: 248 Loss_train: 338879.32907516893, 0:00:00.093092 сек\n",
      "Loss_val: 1103693.625\n",
      "\n",
      "Эпоха: 249 Loss_train: 338394.0516258446, 0:00:00.088202 сек\n",
      "Loss_val: 1101174.625\n",
      "\n",
      "Эпоха: 250 Loss_train: 341286.51863386825, 0:00:00.096040 сек\n",
      "Loss_val: 1098980.625\n",
      "\n",
      "Эпоха: 251 Loss_train: 338705.50733741553, 0:00:00.085150 сек\n",
      "Loss_val: 1096367.125\n",
      "\n",
      "Эпоха: 252 Loss_train: 336641.9458403716, 0:00:00.089196 сек\n",
      "Loss_val: 1093289.625\n",
      "\n",
      "Эпоха: 253 Loss_train: 336347.6556165541, 0:00:00.103620 сек\n",
      "Loss_val: 1090752.875\n",
      "\n",
      "Эпоха: 254 Loss_train: 336711.9861697635, 0:00:00.088576 сек\n",
      "Loss_val: 1088501.375\n",
      "\n",
      "Эпоха: 255 Loss_train: 336036.89933488175, 0:00:00.095656 сек\n",
      "Loss_val: 1085758.0\n",
      "\n",
      "Эпоха: 256 Loss_train: 334971.58878800675, 0:00:00.095996 сек\n",
      "Loss_val: 1082984.375\n",
      "\n",
      "Эпоха: 257 Loss_train: 334451.89944045607, 0:00:00.101044 сек\n",
      "Loss_val: 1080235.75\n",
      "\n",
      "Эпоха: 258 Loss_train: 334079.9641047297, 0:00:00.089996 сек\n",
      "Loss_val: 1077912.875\n",
      "\n",
      "Эпоха: 259 Loss_train: 333799.0075485642, 0:00:00.160169 сек\n",
      "Loss_val: 1075393.625\n",
      "\n",
      "Эпоха: 260 Loss_train: 333223.1291701858, 0:00:00.090636 сек\n",
      "Loss_val: 1072749.375\n",
      "\n",
      "Эпоха: 261 Loss_train: 333109.5019003378, 0:00:00.088299 сек\n",
      "Loss_val: 1070288.375\n",
      "\n",
      "Эпоха: 262 Loss_train: 332239.5983688767, 0:00:00.094208 сек\n",
      "Loss_val: 1067637.25\n",
      "\n",
      "Эпоха: 263 Loss_train: 331405.16110641893, 0:00:00.086512 сек\n",
      "Loss_val: 1065312.25\n",
      "\n",
      "Эпоха: 264 Loss_train: 330953.2207559122, 0:00:00.098014 сек\n",
      "Loss_val: 1062669.125\n",
      "\n",
      "Эпоха: 265 Loss_train: 331138.90593327704, 0:00:00.087191 сек\n",
      "Loss_val: 1060564.125\n",
      "\n",
      "Эпоха: 266 Loss_train: 330286.4549197635, 0:00:00.081998 сек\n",
      "Loss_val: 1057590.875\n",
      "\n",
      "Эпоха: 267 Loss_train: 329520.2558065878, 0:00:00.097540 сек\n",
      "Loss_val: 1055135.875\n",
      "\n",
      "Эпоха: 268 Loss_train: 330681.08504011825, 0:00:00.095218 сек\n",
      "Loss_val: 1052907.5\n",
      "\n",
      "Эпоха: 269 Loss_train: 328483.3627005912, 0:00:00.089123 сек\n",
      "Loss_val: 1050181.375\n",
      "\n",
      "Эпоха: 270 Loss_train: 328903.6212521115, 0:00:00.087594 сек\n",
      "Loss_val: 1047728.0625\n",
      "\n",
      "Эпоха: 271 Loss_train: 328161.2673141892, 0:00:00.102143 сек\n",
      "Loss_val: 1045767.8125\n",
      "\n",
      "Эпоха: 272 Loss_train: 327422.23331925675, 0:00:00.096604 сек\n",
      "Loss_val: 1043000.0625\n",
      "\n",
      "Эпоха: 273 Loss_train: 327718.9426203547, 0:00:00.091647 сек\n",
      "Loss_val: 1040607.3125\n",
      "\n",
      "Эпоха: 274 Loss_train: 326766.7937077703, 0:00:00.089481 сек\n",
      "Loss_val: 1038004.4375\n",
      "\n",
      "Эпоха: 275 Loss_train: 327198.6323902027, 0:00:00.092015 сек\n",
      "Loss_val: 1035399.8125\n",
      "\n",
      "Эпоха: 276 Loss_train: 325380.2062922297, 0:00:00.089144 сек\n",
      "Loss_val: 1032973.6875\n",
      "\n",
      "Эпоха: 277 Loss_train: 325601.69942989864, 0:00:00.082583 сек\n",
      "Loss_val: 1030823.0625\n",
      "\n",
      "Эпоха: 278 Loss_train: 325719.45945945947, 0:00:00.081559 сек\n",
      "Loss_val: 1028521.8125\n",
      "\n",
      "Эпоха: 279 Loss_train: 324393.32390202704, 0:00:00.089151 сек\n",
      "Loss_val: 1025829.5\n",
      "\n",
      "Эпоха: 280 Loss_train: 324669.80225929053, 0:00:00.080101 сек\n",
      "Loss_val: 1023560.0\n",
      "\n",
      "Эпоха: 281 Loss_train: 324130.0520481419, 0:00:00.098574 сек\n",
      "Loss_val: 1020966.0\n",
      "\n",
      "Эпоха: 282 Loss_train: 323078.38972761825, 0:00:00.107375 сек\n",
      "Loss_val: 1018812.0625\n",
      "\n",
      "Эпоха: 283 Loss_train: 322413.9718116554, 0:00:00.089601 сек\n",
      "Loss_val: 1016213.25\n",
      "\n",
      "Эпоха: 284 Loss_train: 322303.9620988176, 0:00:00.086998 сек\n",
      "Loss_val: 1013915.5\n",
      "\n",
      "Эпоха: 285 Loss_train: 322062.07136824325, 0:00:00.090221 сек\n",
      "Loss_val: 1011857.3125\n",
      "\n",
      "Эпоха: 286 Loss_train: 321263.24609375, 0:00:00.097237 сек\n",
      "Loss_val: 1009128.9375\n",
      "\n",
      "Эпоха: 287 Loss_train: 320750.6145481419, 0:00:00.082075 сек\n",
      "Loss_val: 1006748.25\n",
      "\n",
      "Эпоха: 288 Loss_train: 320397.08984375, 0:00:00.084650 сек\n",
      "Loss_val: 1004577.1875\n",
      "\n",
      "Эпоха: 289 Loss_train: 320103.36555109796, 0:00:00.084110 сек\n",
      "Loss_val: 1002221.8125\n",
      "\n",
      "Эпоха: 290 Loss_train: 319874.9832136824, 0:00:00.090572 сек\n",
      "Loss_val: 999918.25\n",
      "\n",
      "Эпоха: 291 Loss_train: 319646.40588048985, 0:00:00.094667 сек\n",
      "Loss_val: 997324.75\n",
      "\n",
      "Эпоха: 292 Loss_train: 318918.71516047296, 0:00:00.094519 сек\n",
      "Loss_val: 995025.6875\n",
      "\n",
      "Эпоха: 293 Loss_train: 318526.5467694257, 0:00:00.101522 сек\n",
      "Loss_val: 992556.0625\n",
      "\n",
      "Эпоха: 294 Loss_train: 318611.80178420607, 0:00:00.084587 сек\n",
      "Loss_val: 990596.3125\n",
      "\n",
      "Эпоха: 295 Loss_train: 317813.60372677364, 0:00:00.090038 сек\n",
      "Loss_val: 988225.5625\n",
      "\n",
      "Эпоха: 296 Loss_train: 317593.67736486485, 0:00:00.098038 сек\n",
      "Loss_val: 986063.6875\n",
      "\n",
      "Эпоха: 297 Loss_train: 317085.2902766047, 0:00:00.091944 сек\n",
      "Loss_val: 984063.75\n",
      "\n",
      "Эпоха: 298 Loss_train: 317833.3942673142, 0:00:00.084658 сек\n",
      "Loss_val: 981624.75\n",
      "\n",
      "Эпоха: 299 Loss_train: 316037.2885346284, 0:00:00.083036 сек\n",
      "Loss_val: 979388.0625\n",
      "\n",
      "Эпоха: 300 Loss_train: 315314.74757179053, 0:00:00.097084 сек\n",
      "Loss_val: 977245.6875\n",
      "\n",
      "Эпоха: 301 Loss_train: 315245.2491554054, 0:00:00.093120 сек\n",
      "Loss_val: 974915.25\n",
      "\n",
      "Эпоха: 302 Loss_train: 314316.9308488176, 0:00:00.108788 сек\n",
      "Loss_val: 972414.4375\n",
      "\n",
      "Эпоха: 303 Loss_train: 314902.6984797297, 0:00:00.099647 сек\n",
      "Loss_val: 970923.5625\n",
      "\n",
      "Эпоха: 304 Loss_train: 313546.6055215372, 0:00:00.086595 сек\n",
      "Loss_val: 968362.5\n",
      "\n",
      "Эпоха: 305 Loss_train: 313514.99118454393, 0:00:00.096886 сек\n",
      "Loss_val: 966229.8125\n",
      "\n",
      "Эпоха: 306 Loss_train: 313445.2240815034, 0:00:00.098132 сек\n",
      "Loss_val: 963968.3125\n",
      "\n",
      "Эпоха: 307 Loss_train: 312765.5778610642, 0:00:00.078605 сек\n",
      "Loss_val: 961618.1875\n",
      "\n",
      "Эпоха: 308 Loss_train: 312685.70877322636, 0:00:00.080094 сек\n",
      "Loss_val: 959609.75\n",
      "\n",
      "Эпоха: 309 Loss_train: 312468.4408783784, 0:00:00.103999 сек\n",
      "Loss_val: 957547.6875\n",
      "\n",
      "Эпоха: 310 Loss_train: 312273.4459987331, 0:00:00.094032 сек\n",
      "Loss_val: 955487.3125\n",
      "\n",
      "Эпоха: 311 Loss_train: 311545.19246199325, 0:00:00.091177 сек\n",
      "Loss_val: 953406.25\n",
      "\n",
      "Эпоха: 312 Loss_train: 310999.09480574325, 0:00:00.093636 сек\n",
      "Loss_val: 951156.0625\n",
      "\n",
      "Эпоха: 313 Loss_train: 310473.79064611485, 0:00:00.082542 сек\n",
      "Loss_val: 948952.1875\n",
      "\n",
      "Эпоха: 314 Loss_train: 310561.83044763515, 0:00:00.079640 сек\n",
      "Loss_val: 946755.5625\n",
      "\n",
      "Эпоха: 315 Loss_train: 310266.8047930743, 0:00:00.086045 сек\n",
      "Loss_val: 944442.9375\n",
      "\n",
      "Эпоха: 316 Loss_train: 309595.32791385136, 0:00:00.093115 сек\n",
      "Loss_val: 942728.25\n",
      "\n",
      "Эпоха: 317 Loss_train: 309056.9152766047, 0:00:00.099117 сек\n",
      "Loss_val: 940416.8125\n",
      "\n",
      "Эпоха: 318 Loss_train: 308505.7864759291, 0:00:00.103184 сек\n",
      "Loss_val: 938173.5625\n",
      "\n",
      "Эпоха: 319 Loss_train: 308659.7667863176, 0:00:00.099962 сек\n",
      "Loss_val: 936459.75\n",
      "\n",
      "Эпоха: 320 Loss_train: 307814.6888196791, 0:00:00.099037 сек\n",
      "Loss_val: 934223.5625\n",
      "\n",
      "Эпоха: 321 Loss_train: 307780.4019214527, 0:00:00.094200 сек\n",
      "Loss_val: 932207.9375\n",
      "\n",
      "Эпоха: 322 Loss_train: 307338.6712943412, 0:00:00.090695 сек\n",
      "Loss_val: 929992.3125\n",
      "\n",
      "Эпоха: 323 Loss_train: 307292.9597761824, 0:00:00.103138 сек\n",
      "Loss_val: 927917.0625\n",
      "\n",
      "Эпоха: 324 Loss_train: 306560.2804581926, 0:00:00.101082 сек\n",
      "Loss_val: 925987.25\n",
      "\n",
      "Эпоха: 325 Loss_train: 307309.2284628378, 0:00:00.100125 сек\n",
      "Loss_val: 923899.1875\n",
      "\n",
      "Эпоха: 326 Loss_train: 306213.5235430743, 0:00:00.106397 сек\n",
      "Loss_val: 921909.1875\n",
      "\n",
      "Эпоха: 327 Loss_train: 305238.4660050676, 0:00:00.106291 сек\n",
      "Loss_val: 919628.9375\n",
      "\n",
      "Эпоха: 328 Loss_train: 305353.7755489865, 0:00:00.094756 сек\n",
      "Loss_val: 917938.9375\n",
      "\n",
      "Эпоха: 329 Loss_train: 304461.05632390204, 0:00:00.091516 сек\n",
      "Loss_val: 915638.5\n",
      "\n",
      "Эпоха: 330 Loss_train: 304221.60705236485, 0:00:00.086122 сек\n",
      "Loss_val: 913459.75\n",
      "\n",
      "Эпоха: 331 Loss_train: 303858.72049197636, 0:00:00.095590 сек\n",
      "Loss_val: 911578.6875\n",
      "\n",
      "Эпоха: 332 Loss_train: 303802.53410050675, 0:00:00.088574 сек\n",
      "Loss_val: 909822.1875\n",
      "\n",
      "Эпоха: 333 Loss_train: 303228.50295608107, 0:00:00.100583 сек\n",
      "Loss_val: 907494.5625\n",
      "\n",
      "Эпоха: 334 Loss_train: 303096.25242820947, 0:00:00.091105 сек\n",
      "Loss_val: 905885.0\n",
      "\n",
      "Эпоха: 335 Loss_train: 302608.6068412162, 0:00:00.098901 сек\n",
      "Loss_val: 903862.8125\n",
      "\n",
      "Эпоха: 336 Loss_train: 303032.60900548985, 0:00:00.095456 сек\n",
      "Loss_val: 901926.8125\n",
      "\n",
      "Эпоха: 337 Loss_train: 301717.21394636825, 0:00:00.095327 сек\n",
      "Loss_val: 899700.0\n",
      "\n",
      "Эпоха: 338 Loss_train: 301544.38048986485, 0:00:00.098701 сек\n",
      "Loss_val: 897818.5625\n",
      "\n",
      "Эпоха: 339 Loss_train: 301238.8567884291, 0:00:00.090802 сек\n",
      "Loss_val: 896199.1875\n",
      "\n",
      "Эпоха: 340 Loss_train: 301201.7339527027, 0:00:00.080648 сек\n",
      "Loss_val: 894142.0\n",
      "\n",
      "Эпоха: 341 Loss_train: 300446.79513302364, 0:00:00.090617 сек\n",
      "Loss_val: 892316.6875\n",
      "\n",
      "Эпоха: 342 Loss_train: 300822.8026288007, 0:00:00.099127 сек\n",
      "Loss_val: 890366.5\n",
      "\n",
      "Эпоха: 343 Loss_train: 300764.7918602196, 0:00:00.099406 сек\n",
      "Loss_val: 888466.1875\n",
      "\n",
      "Эпоха: 344 Loss_train: 300589.6941511824, 0:00:00.092306 сек\n",
      "Loss_val: 886527.8125\n",
      "\n",
      "Эпоха: 345 Loss_train: 299171.64025548985, 0:00:00.092765 сек\n",
      "Loss_val: 884228.5625\n",
      "\n",
      "Эпоха: 346 Loss_train: 299276.02829391893, 0:00:00.082239 сек\n",
      "Loss_val: 882465.75\n",
      "\n",
      "Эпоха: 347 Loss_train: 298831.7564928209, 0:00:00.083141 сек\n",
      "Loss_val: 880665.0\n",
      "\n",
      "Эпоха: 348 Loss_train: 299406.5176836993, 0:00:00.095143 сек\n",
      "Loss_val: 878832.8125\n",
      "\n",
      "Эпоха: 349 Loss_train: 298413.5057010135, 0:00:00.079145 сек\n",
      "Loss_val: 877046.4375\n",
      "\n",
      "Эпоха: 350 Loss_train: 297830.2636190878, 0:00:00.095086 сек\n",
      "Loss_val: 874775.75\n",
      "\n",
      "Эпоха: 351 Loss_train: 297503.8997571791, 0:00:00.113285 сек\n",
      "Loss_val: 872923.75\n",
      "\n",
      "Эпоха: 352 Loss_train: 297311.21827491553, 0:00:00.098683 сек\n",
      "Loss_val: 871241.5625\n",
      "\n",
      "Эпоха: 353 Loss_train: 297893.80320945947, 0:00:00.098774 сек\n",
      "Loss_val: 869682.0625\n",
      "\n",
      "Эпоха: 354 Loss_train: 296499.73205236485, 0:00:00.099132 сек\n",
      "Loss_val: 867611.6875\n",
      "\n",
      "Эпоха: 355 Loss_train: 296482.82780827704, 0:00:00.095109 сек\n",
      "Loss_val: 865608.3125\n",
      "\n",
      "Эпоха: 356 Loss_train: 296193.8709881757, 0:00:00.091596 сек\n",
      "Loss_val: 863808.3125\n",
      "\n",
      "Эпоха: 357 Loss_train: 295757.4041385135, 0:00:00.094618 сек\n",
      "Loss_val: 862378.3125\n",
      "\n",
      "Эпоха: 358 Loss_train: 295245.8243771115, 0:00:00.092303 сек\n",
      "Loss_val: 860110.0625\n",
      "\n",
      "Эпоха: 359 Loss_train: 295587.68976984796, 0:00:00.101697 сек\n",
      "Loss_val: 858519.5625\n",
      "\n",
      "Эпоха: 360 Loss_train: 294839.9014991554, 0:00:00.097120 сек\n",
      "Loss_val: 857077.1875\n",
      "\n",
      "Эпоха: 361 Loss_train: 295032.21114864864, 0:00:00.084067 сек\n",
      "Loss_val: 855394.3125\n",
      "\n",
      "Эпоха: 362 Loss_train: 294528.09744510136, 0:00:00.080767 сек\n",
      "Loss_val: 853603.4375\n",
      "\n",
      "Эпоха: 363 Loss_train: 294009.2422930743, 0:00:00.095675 сек\n",
      "Loss_val: 851851.5\n",
      "\n",
      "Эпоха: 364 Loss_train: 294171.5372677365, 0:00:00.091609 сек\n",
      "Loss_val: 850403.3125\n",
      "\n",
      "Эпоха: 365 Loss_train: 294091.0573796453, 0:00:00.090629 сек\n",
      "Loss_val: 847986.9375\n",
      "\n",
      "Эпоха: 366 Loss_train: 293331.91316511825, 0:00:00.102143 сек\n",
      "Loss_val: 846179.5625\n",
      "\n",
      "Эпоха: 367 Loss_train: 292741.5038534628, 0:00:00.079100 сек\n",
      "Loss_val: 844724.0\n",
      "\n",
      "Эпоха: 368 Loss_train: 293124.14358108107, 0:00:00.088573 сек\n",
      "Loss_val: 842951.3125\n",
      "\n",
      "Эпоха: 369 Loss_train: 292289.1418918919, 0:00:00.087079 сек\n",
      "Loss_val: 841172.5\n",
      "\n",
      "Эпоха: 370 Loss_train: 292704.8743137669, 0:00:00.092077 сек\n",
      "Loss_val: 839837.8125\n",
      "\n",
      "Эпоха: 371 Loss_train: 291590.5959406672, 0:00:00.078266 сек\n",
      "Loss_val: 837749.4375\n",
      "\n",
      "Эпоха: 372 Loss_train: 291561.3673458615, 0:00:00.077719 сек\n",
      "Loss_val: 836078.25\n",
      "\n",
      "Эпоха: 373 Loss_train: 291595.6834353885, 0:00:00.074623 сек\n",
      "Loss_val: 834550.5\n",
      "\n",
      "Эпоха: 374 Loss_train: 291063.28161951015, 0:00:00.089669 сек\n",
      "Loss_val: 832603.5\n",
      "\n",
      "Эпоха: 375 Loss_train: 293953.5376372466, 0:00:00.098135 сек\n",
      "Loss_val: 831151.5625\n",
      "\n",
      "Эпоха: 376 Loss_train: 290450.8332981419, 0:00:00.097040 сек\n",
      "Loss_val: 829169.9375\n",
      "\n",
      "Эпоха: 377 Loss_train: 290322.56788429053, 0:00:00.100585 сек\n",
      "Loss_val: 827582.1875\n",
      "\n",
      "Эпоха: 378 Loss_train: 290346.22941300675, 0:00:00.090109 сек\n",
      "Loss_val: 826187.0\n",
      "\n",
      "Эпоха: 379 Loss_train: 289827.08614864864, 0:00:00.089033 сек\n",
      "Loss_val: 824257.4375\n",
      "\n",
      "Эпоха: 380 Loss_train: 290189.4084670608, 0:00:00.091084 сек\n",
      "Loss_val: 822706.5\n",
      "\n",
      "Эпоха: 381 Loss_train: 289123.6447951858, 0:00:00.086819 сек\n",
      "Loss_val: 821306.1875\n",
      "\n",
      "Эпоха: 382 Loss_train: 289002.97775021114, 0:00:00.089267 сек\n",
      "Loss_val: 819772.8125\n",
      "\n",
      "Эпоха: 383 Loss_train: 288892.1143633868, 0:00:00.081117 сек\n",
      "Loss_val: 818260.6875\n",
      "\n",
      "Эпоха: 384 Loss_train: 288792.47265625, 0:00:00.098085 сек\n",
      "Loss_val: 816339.6875\n",
      "\n",
      "Эпоха: 385 Loss_train: 288316.6406777872, 0:00:00.099596 сек\n",
      "Loss_val: 815045.5\n",
      "\n",
      "Эпоха: 386 Loss_train: 288113.1875, 0:00:00.081106 сек\n",
      "Loss_val: 813334.25\n",
      "\n",
      "Эпоха: 387 Loss_train: 287488.9327227618, 0:00:00.087206 сек\n",
      "Loss_val: 811794.75\n",
      "\n",
      "Эпоха: 388 Loss_train: 288271.6797930743, 0:00:00.084603 сек\n",
      "Loss_val: 810414.25\n",
      "\n",
      "Эпоха: 389 Loss_train: 287707.0675147804, 0:00:00.099123 сек\n",
      "Loss_val: 808880.1875\n",
      "\n",
      "Эпоха: 390 Loss_train: 287364.80949113175, 0:00:00.095223 сек\n",
      "Loss_val: 807284.3125\n",
      "\n",
      "Эпоха: 391 Loss_train: 286707.2968222128, 0:00:00.079109 сек\n",
      "Loss_val: 805476.25\n",
      "\n",
      "Эпоха: 392 Loss_train: 287460.6535050676, 0:00:00.083652 сек\n",
      "Loss_val: 804276.1875\n",
      "\n",
      "Эпоха: 393 Loss_train: 286531.5137246622, 0:00:00.091558 сек\n",
      "Loss_val: 802694.4375\n",
      "\n",
      "Эпоха: 394 Loss_train: 286054.1121727196, 0:00:00.080117 сек\n",
      "Loss_val: 801010.8125\n",
      "\n",
      "Эпоха: 395 Loss_train: 285880.42039695947, 0:00:00.094331 сек\n",
      "Loss_val: 799431.75\n",
      "\n",
      "Эпоха: 396 Loss_train: 286099.79793074325, 0:00:00.098237 сек\n",
      "Loss_val: 798128.75\n",
      "\n",
      "Эпоха: 397 Loss_train: 286429.65720016893, 0:00:00.093567 сек\n",
      "Loss_val: 797021.75\n",
      "\n",
      "Эпоха: 398 Loss_train: 285093.6310177365, 0:00:00.092917 сек\n",
      "Loss_val: 795150.75\n",
      "\n",
      "Эпоха: 399 Loss_train: 285025.6028821791, 0:00:00.093042 сек\n",
      "Loss_val: 793805.6875\n",
      "\n",
      "Эпоха: 400 Loss_train: 284841.1275337838, 0:00:00.103041 сек\n",
      "Loss_val: 792369.6875\n",
      "\n",
      "Эпоха: 401 Loss_train: 287648.8253800676, 0:00:00.084106 сек\n",
      "Loss_val: 790807.5625\n",
      "\n",
      "Эпоха: 402 Loss_train: 284672.6548247466, 0:00:00.098145 сек\n",
      "Loss_val: 789179.0\n",
      "\n",
      "Эпоха: 403 Loss_train: 284422.09670608107, 0:00:00.096600 сек\n",
      "Loss_val: 787820.5625\n",
      "\n",
      "Эпоха: 404 Loss_train: 284126.4194995777, 0:00:00.101302 сек\n",
      "Loss_val: 786381.9375\n",
      "\n",
      "Эпоха: 405 Loss_train: 284015.0760663007, 0:00:00.108540 сек\n",
      "Loss_val: 784914.25\n",
      "\n",
      "Эпоха: 406 Loss_train: 283829.2956081081, 0:00:00.106803 сек\n",
      "Loss_val: 783546.3125\n",
      "\n",
      "Эпоха: 407 Loss_train: 283556.9003378378, 0:00:00.100159 сек\n",
      "Loss_val: 782099.0\n",
      "\n",
      "Эпоха: 408 Loss_train: 283238.103515625, 0:00:00.084078 сек\n",
      "Loss_val: 780563.5\n",
      "\n",
      "Эпоха: 409 Loss_train: 283022.06397804053, 0:00:00.100131 сек\n",
      "Loss_val: 779363.9375\n",
      "\n",
      "Эпоха: 410 Loss_train: 282631.0377428209, 0:00:00.089109 сек\n",
      "Loss_val: 777704.5\n",
      "\n",
      "Эпоха: 411 Loss_train: 282939.09591427364, 0:00:00.088560 сек\n",
      "Loss_val: 776760.3125\n",
      "\n",
      "Эпоха: 412 Loss_train: 282746.0443940034, 0:00:00.098164 сек\n",
      "Loss_val: 775028.5\n",
      "\n",
      "Эпоха: 413 Loss_train: 282676.4229835304, 0:00:00.084572 сек\n",
      "Loss_val: 773738.25\n",
      "\n",
      "Эпоха: 414 Loss_train: 281643.7237647804, 0:00:00.083106 сек\n",
      "Loss_val: 772301.0\n",
      "\n",
      "Эпоха: 415 Loss_train: 282281.7576541385, 0:00:00.097620 сек\n",
      "Loss_val: 771303.5\n",
      "\n",
      "Эпоха: 416 Loss_train: 281542.7943412162, 0:00:00.103861 сек\n",
      "Loss_val: 769443.4375\n",
      "\n",
      "Эпоха: 417 Loss_train: 281631.3131862331, 0:00:00.093002 сек\n",
      "Loss_val: 768261.1875\n",
      "\n",
      "Эпоха: 418 Loss_train: 281341.83361486485, 0:00:00.084355 сек\n",
      "Loss_val: 766816.25\n",
      "\n",
      "Эпоха: 419 Loss_train: 281257.0233319257, 0:00:00.082639 сек\n",
      "Loss_val: 765477.8125\n",
      "\n",
      "Эпоха: 420 Loss_train: 280754.7624049831, 0:00:00.093174 сек\n",
      "Loss_val: 763834.6875\n",
      "\n",
      "Эпоха: 421 Loss_train: 280498.0506228885, 0:00:00.084150 сек\n",
      "Loss_val: 762816.0625\n",
      "\n",
      "Эпоха: 422 Loss_train: 280515.8072212838, 0:00:00.090996 сек\n",
      "Loss_val: 761427.0625\n",
      "\n",
      "Эпоха: 423 Loss_train: 280351.8984375, 0:00:00.093037 сек\n",
      "Loss_val: 760056.0\n",
      "\n",
      "Эпоха: 424 Loss_train: 280540.4616765203, 0:00:00.083072 сек\n",
      "Loss_val: 758762.5\n",
      "\n",
      "Эпоха: 425 Loss_train: 280174.6112753378, 0:00:00.093597 сек\n",
      "Loss_val: 757404.0\n",
      "\n",
      "Эпоха: 426 Loss_train: 279691.9458931588, 0:00:00.088674 сек\n",
      "Loss_val: 756192.9375\n",
      "\n",
      "Эпоха: 427 Loss_train: 279822.93507179053, 0:00:00.085592 сек\n",
      "Loss_val: 754854.0\n",
      "\n",
      "Эпоха: 428 Loss_train: 279672.9387141047, 0:00:00.092046 сек\n",
      "Loss_val: 753704.75\n",
      "\n",
      "Эпоха: 429 Loss_train: 279101.9359691723, 0:00:00.115130 сек\n",
      "Loss_val: 752316.25\n",
      "\n",
      "Эпоха: 430 Loss_train: 279044.3033150338, 0:00:00.101211 сек\n",
      "Loss_val: 750810.6875\n",
      "\n",
      "Эпоха: 431 Loss_train: 278673.6148120777, 0:00:00.109212 сек\n",
      "Loss_val: 749624.1875\n",
      "\n",
      "Эпоха: 432 Loss_train: 279290.0026921453, 0:00:00.108126 сек\n",
      "Loss_val: 748499.3125\n",
      "\n",
      "Эпоха: 433 Loss_train: 278536.96769425675, 0:00:00.092039 сек\n",
      "Loss_val: 747217.5\n",
      "\n",
      "Эпоха: 434 Loss_train: 278123.90128800675, 0:00:00.102132 сек\n",
      "Loss_val: 745838.0625\n",
      "\n",
      "Эпоха: 435 Loss_train: 278047.48474451015, 0:00:00.099627 сек\n",
      "Loss_val: 744483.0\n",
      "\n",
      "Эпоха: 436 Loss_train: 278276.0253378378, 0:00:00.098628 сек\n",
      "Loss_val: 743771.4375\n",
      "\n",
      "Эпоха: 437 Loss_train: 277798.60124577704, 0:00:00.084094 сек\n",
      "Loss_val: 742258.9375\n",
      "\n",
      "Эпоха: 438 Loss_train: 278057.5005806588, 0:00:00.083088 сек\n",
      "Loss_val: 741050.75\n",
      "\n",
      "Эпоха: 439 Loss_train: 277256.5069679054, 0:00:00.093111 сек\n",
      "Loss_val: 739469.75\n",
      "\n",
      "Эпоха: 440 Loss_train: 277255.8912584459, 0:00:00.078668 сек\n",
      "Loss_val: 738505.4375\n",
      "\n",
      "Эпоха: 441 Loss_train: 277234.69441511825, 0:00:00.085707 сек\n",
      "Loss_val: 737392.4375\n",
      "\n",
      "Эпоха: 442 Loss_train: 276815.93649704393, 0:00:00.083943 сек\n",
      "Loss_val: 735689.4375\n",
      "\n",
      "Эпоха: 443 Loss_train: 276860.8346706081, 0:00:00.083227 сек\n",
      "Loss_val: 734884.5\n",
      "\n",
      "Эпоха: 444 Loss_train: 276565.0599134291, 0:00:00.081578 сек\n",
      "Loss_val: 733846.75\n",
      "\n",
      "Эпоха: 445 Loss_train: 276981.83240076015, 0:00:00.091970 сек\n",
      "Loss_val: 732673.9375\n",
      "\n",
      "Эпоха: 446 Loss_train: 275989.0943042652, 0:00:00.095585 сек\n",
      "Loss_val: 731026.5\n",
      "\n",
      "Эпоха: 447 Loss_train: 277148.0230152027, 0:00:00.080322 сек\n",
      "Loss_val: 730320.4375\n",
      "\n",
      "Эпоха: 448 Loss_train: 276270.0460304054, 0:00:00.086207 сек\n",
      "Loss_val: 728755.5625\n",
      "\n",
      "Эпоха: 449 Loss_train: 276282.6069995777, 0:00:00.095094 сек\n",
      "Loss_val: 727894.75\n",
      "\n",
      "Эпоха: 450 Loss_train: 275489.77734375, 0:00:00.089068 сек\n",
      "Loss_val: 726623.5625\n",
      "\n",
      "Эпоха: 451 Loss_train: 275408.53663429053, 0:00:00.094089 сек\n",
      "Loss_val: 725296.4375\n",
      "\n",
      "Эпоха: 452 Loss_train: 276116.1255806588, 0:00:00.091105 сек\n",
      "Loss_val: 724486.5625\n",
      "\n",
      "Эпоха: 453 Loss_train: 275285.2077174831, 0:00:00.087150 сек\n",
      "Loss_val: 722922.5625\n",
      "\n",
      "Эпоха: 454 Loss_train: 275597.94763513515, 0:00:00.093629 сек\n",
      "Loss_val: 721910.75\n",
      "\n",
      "Эпоха: 455 Loss_train: 274886.6074746622, 0:00:00.093201 сек\n",
      "Loss_val: 720937.9375\n",
      "\n",
      "Эпоха: 456 Loss_train: 275823.5782305743, 0:00:00.092589 сек\n",
      "Loss_val: 719832.75\n",
      "\n",
      "Эпоха: 457 Loss_train: 274395.1388302365, 0:00:00.098855 сек\n",
      "Loss_val: 718461.25\n",
      "\n",
      "Эпоха: 458 Loss_train: 275066.4256756757, 0:00:00.103536 сек\n",
      "Loss_val: 717734.6875\n",
      "\n",
      "Эпоха: 459 Loss_train: 274181.3574746622, 0:00:00.093266 сек\n",
      "Loss_val: 716430.8125\n",
      "\n",
      "Эпоха: 460 Loss_train: 274525.3424303209, 0:00:00.096552 сек\n",
      "Loss_val: 715266.9375\n",
      "\n",
      "Эпоха: 461 Loss_train: 274164.9129539696, 0:00:00.089039 сек\n",
      "Loss_val: 714146.75\n",
      "\n",
      "Эпоха: 462 Loss_train: 273858.26066300675, 0:00:00.095036 сек\n",
      "Loss_val: 713310.5\n",
      "\n",
      "Эпоха: 463 Loss_train: 273759.58292863175, 0:00:00.090059 сек\n",
      "Loss_val: 712340.0\n",
      "\n",
      "Эпоха: 464 Loss_train: 274144.9979940878, 0:00:00.089002 сек\n",
      "Loss_val: 711089.3125\n",
      "\n",
      "Эпоха: 465 Loss_train: 274220.1640625, 0:00:00.088091 сек\n",
      "Loss_val: 710214.9375\n",
      "\n",
      "Эпоха: 466 Loss_train: 273248.40281883447, 0:00:00.104064 сек\n",
      "Loss_val: 708767.3125\n",
      "\n",
      "Эпоха: 467 Loss_train: 273457.2135768581, 0:00:00.089092 сек\n",
      "Loss_val: 707662.4375\n",
      "\n",
      "Эпоха: 468 Loss_train: 273203.4926097973, 0:00:00.082118 сек\n",
      "Loss_val: 706857.4375\n",
      "\n",
      "Эпоха: 469 Loss_train: 272888.0919552365, 0:00:00.092569 сек\n",
      "Loss_val: 705896.8125\n",
      "\n",
      "Эпоха: 470 Loss_train: 272892.892578125, 0:00:00.084217 сек\n",
      "Loss_val: 704377.9375\n",
      "\n",
      "Эпоха: 471 Loss_train: 275516.9187605574, 0:00:00.078141 сек\n",
      "Loss_val: 703542.4375\n",
      "\n",
      "Эпоха: 472 Loss_train: 272329.4557907517, 0:00:00.096590 сек\n",
      "Loss_val: 702436.9375\n",
      "\n",
      "Эпоха: 473 Loss_train: 272483.5464527027, 0:00:00.083270 сек\n",
      "Loss_val: 701186.1875\n",
      "\n",
      "Эпоха: 474 Loss_train: 272015.81661739864, 0:00:00.099678 сек\n",
      "Loss_val: 700501.3125\n",
      "\n",
      "Эпоха: 475 Loss_train: 272132.0382179054, 0:00:00.098283 сек\n",
      "Loss_val: 699515.1875\n",
      "\n",
      "Эпоха: 476 Loss_train: 271811.8748416385, 0:00:00.090033 сек\n",
      "Loss_val: 698365.625\n",
      "\n",
      "Эпоха: 477 Loss_train: 271788.5350506757, 0:00:00.098433 сек\n",
      "Loss_val: 697273.1875\n",
      "\n",
      "Эпоха: 478 Loss_train: 272160.1929370777, 0:00:00.107245 сек\n",
      "Loss_val: 696445.4375\n",
      "\n",
      "Эпоха: 479 Loss_train: 272619.5708931588, 0:00:00.091924 сек\n",
      "Loss_val: 695616.8125\n",
      "\n",
      "Эпоха: 480 Loss_train: 271354.36892947636, 0:00:00.079523 сек\n",
      "Loss_val: 694409.0625\n",
      "\n",
      "Эпоха: 481 Loss_train: 271367.1834881757, 0:00:00.098070 сек\n",
      "Loss_val: 693194.1875\n",
      "\n",
      "Эпоха: 482 Loss_train: 271270.4120565878, 0:00:00.090037 сек\n",
      "Loss_val: 692399.375\n",
      "\n",
      "Эпоха: 483 Loss_train: 271788.7557538007, 0:00:00.082076 сек\n",
      "Loss_val: 691469.4375\n",
      "\n",
      "Эпоха: 484 Loss_train: 270964.9250950169, 0:00:00.097553 сек\n",
      "Loss_val: 690645.875\n",
      "\n",
      "Эпоха: 485 Loss_train: 270796.2896431588, 0:00:00.089182 сек\n",
      "Loss_val: 689385.5625\n",
      "\n",
      "Эпоха: 486 Loss_train: 270784.1576752534, 0:00:00.095156 сек\n",
      "Loss_val: 688273.625\n",
      "\n",
      "Эпоха: 487 Loss_train: 270587.2734375, 0:00:00.120209 сек\n",
      "Loss_val: 687232.375\n",
      "\n",
      "Эпоха: 488 Loss_train: 270797.20460304053, 0:00:00.104260 сек\n",
      "Loss_val: 686227.125\n",
      "\n",
      "Эпоха: 489 Loss_train: 270390.4075696791, 0:00:00.101763 сек\n",
      "Loss_val: 685777.0\n",
      "\n",
      "Эпоха: 490 Loss_train: 270283.86349239864, 0:00:00.100678 сек\n",
      "Loss_val: 684554.0625\n",
      "\n",
      "Эпоха: 491 Loss_train: 270665.77042863175, 0:00:00.096545 сек\n",
      "Loss_val: 683667.0625\n",
      "\n",
      "Эпоха: 492 Loss_train: 269922.2114653716, 0:00:00.082210 сек\n",
      "Loss_val: 682734.3125\n",
      "\n",
      "Эпоха: 493 Loss_train: 270185.8868771115, 0:00:00.084157 сек\n",
      "Loss_val: 682063.75\n",
      "\n",
      "Эпоха: 494 Loss_train: 269648.9788059544, 0:00:00.093549 сек\n",
      "Loss_val: 680995.75\n",
      "\n",
      "Эпоха: 495 Loss_train: 269663.72957136825, 0:00:00.095652 сек\n",
      "Loss_val: 679662.6875\n",
      "\n",
      "Эпоха: 496 Loss_train: 269855.95349451015, 0:00:00.096564 сек\n",
      "Loss_val: 679272.1875\n",
      "\n",
      "Эпоха: 497 Loss_train: 269645.3339315878, 0:00:00.095207 сек\n",
      "Loss_val: 678282.0625\n",
      "\n",
      "Эпоха: 498 Loss_train: 269296.0489600929, 0:00:00.082552 сек\n",
      "Loss_val: 677004.4375\n",
      "\n",
      "Эпоха: 499 Loss_train: 269649.28621199325, 0:00:00.085589 сек\n",
      "Loss_val: 676329.6875\n",
      "\n",
      "Эпоха: 500 Loss_train: 269092.1138091216, 0:00:00.096664 сек\n",
      "Loss_val: 675293.0625\n",
      "\n",
      "Эпоха: 501 Loss_train: 268878.6182960304, 0:00:00.096143 сек\n",
      "Loss_val: 674323.1875\n",
      "\n",
      "Эпоха: 502 Loss_train: 269458.9480574324, 0:00:00.095144 сек\n",
      "Loss_val: 673861.9375\n",
      "\n",
      "Эпоха: 503 Loss_train: 268796.97107263515, 0:00:00.103602 сек\n",
      "Loss_val: 672727.3125\n",
      "\n",
      "Эпоха: 504 Loss_train: 269168.6277977196, 0:00:00.085143 сек\n",
      "Loss_val: 671692.125\n",
      "\n",
      "Эпоха: 505 Loss_train: 268932.0911634291, 0:00:00.098143 сек\n",
      "Loss_val: 670771.0\n",
      "\n",
      "Эпоха: 506 Loss_train: 268483.8273859797, 0:00:00.089187 сек\n",
      "Loss_val: 670038.625\n",
      "\n",
      "Эпоха: 507 Loss_train: 268276.5583298142, 0:00:00.080157 сек\n",
      "Loss_val: 669239.6875\n",
      "\n",
      "Эпоха: 508 Loss_train: 268255.7113597973, 0:00:00.095744 сек\n",
      "Loss_val: 668224.0625\n",
      "\n",
      "Эпоха: 509 Loss_train: 268378.2747571791, 0:00:00.096159 сек\n",
      "Loss_val: 667685.5\n",
      "\n",
      "Эпоха: 510 Loss_train: 268469.2364336993, 0:00:00.096334 сек\n",
      "Loss_val: 666928.8125\n",
      "\n",
      "Эпоха: 511 Loss_train: 267896.1369035051, 0:00:00.085008 сек\n",
      "Loss_val: 665893.3125\n",
      "\n",
      "Эпоха: 512 Loss_train: 268104.4134818412, 0:00:00.093158 сек\n",
      "Loss_val: 665138.375\n",
      "\n",
      "Эпоха: 513 Loss_train: 267850.4227195946, 0:00:00.083366 сек\n",
      "Loss_val: 664196.0625\n",
      "\n",
      "Эпоха: 514 Loss_train: 267748.3649704392, 0:00:00.084209 сек\n",
      "Loss_val: 663583.0\n",
      "\n",
      "Эпоха: 515 Loss_train: 267789.17863175675, 0:00:00.081421 сек\n",
      "Loss_val: 662866.8125\n",
      "\n",
      "Эпоха: 516 Loss_train: 267767.2180109797, 0:00:00.091179 сек\n",
      "Loss_val: 661788.3125\n",
      "\n",
      "Эпоха: 517 Loss_train: 268132.23933699325, 0:00:00.099135 сек\n",
      "Loss_val: 660840.5\n",
      "\n",
      "Эпоха: 518 Loss_train: 267291.5574852196, 0:00:00.098745 сек\n",
      "Loss_val: 660044.625\n",
      "\n",
      "Эпоха: 519 Loss_train: 267277.67873733107, 0:00:00.088463 сек\n",
      "Loss_val: 659330.0625\n",
      "\n",
      "Эпоха: 520 Loss_train: 269856.94494298985, 0:00:00.095585 сек\n",
      "Loss_val: 658457.25\n",
      "\n",
      "Эпоха: 521 Loss_train: 267179.51472761825, 0:00:00.089051 сек\n",
      "Loss_val: 657760.5625\n",
      "\n",
      "Эпоха: 522 Loss_train: 266896.39812077704, 0:00:00.091413 сек\n",
      "Loss_val: 656684.75\n",
      "\n",
      "Эпоха: 523 Loss_train: 266864.005859375, 0:00:00.077709 сек\n",
      "Loss_val: 656137.75\n",
      "\n",
      "Эпоха: 524 Loss_train: 267154.67039695947, 0:00:00.093091 сек\n",
      "Loss_val: 655221.3125\n",
      "\n",
      "Эпоха: 525 Loss_train: 266797.8861908784, 0:00:00.077000 сек\n",
      "Loss_val: 654361.375\n",
      "\n",
      "Эпоха: 526 Loss_train: 266993.0841163429, 0:00:00.088063 сек\n",
      "Loss_val: 653740.4375\n",
      "\n",
      "Эпоха: 527 Loss_train: 266646.9427787162, 0:00:00.097087 сек\n",
      "Loss_val: 652965.25\n",
      "\n",
      "Эпоха: 528 Loss_train: 266668.03059016046, 0:00:00.098609 сек\n",
      "Loss_val: 652067.5\n",
      "\n",
      "Эпоха: 529 Loss_train: 266598.4597233953, 0:00:00.083072 сек\n",
      "Loss_val: 651442.125\n",
      "\n",
      "Эпоха: 530 Loss_train: 266572.16891891893, 0:00:00.101369 сек\n",
      "Loss_val: 650592.125\n",
      "\n",
      "Эпоха: 531 Loss_train: 266702.8728885135, 0:00:00.082657 сек\n",
      "Loss_val: 649745.625\n",
      "\n",
      "Эпоха: 532 Loss_train: 266097.5753800676, 0:00:00.096242 сек\n",
      "Loss_val: 648711.3125\n",
      "\n",
      "Эпоха: 533 Loss_train: 265975.8933699324, 0:00:00.099209 сек\n",
      "Loss_val: 648273.9375\n",
      "\n",
      "Эпоха: 534 Loss_train: 265948.2840477196, 0:00:00.100177 сек\n",
      "Loss_val: 647575.4375\n",
      "\n",
      "Эпоха: 535 Loss_train: 266080.92345861485, 0:00:00.094114 сек\n",
      "Loss_val: 646724.5625\n",
      "\n",
      "Эпоха: 536 Loss_train: 265849.85652449325, 0:00:00.073549 сек\n",
      "Loss_val: 645952.0\n",
      "\n",
      "Эпоха: 537 Loss_train: 266536.67235008447, 0:00:00.091628 сек\n",
      "Loss_val: 645403.75\n",
      "\n",
      "Эпоха: 538 Loss_train: 265543.79391891893, 0:00:00.094698 сек\n",
      "Loss_val: 644507.875\n",
      "\n",
      "Эпоха: 539 Loss_train: 266126.04275760136, 0:00:00.082554 сек\n",
      "Loss_val: 643448.5625\n",
      "\n",
      "Эпоха: 540 Loss_train: 265441.06218327704, 0:00:00.102631 сек\n",
      "Loss_val: 643074.6875\n",
      "\n",
      "Эпоха: 541 Loss_train: 265325.5492504223, 0:00:00.108591 сек\n",
      "Loss_val: 642395.3125\n",
      "\n",
      "Эпоха: 542 Loss_train: 265693.8628589527, 0:00:00.102073 сек\n",
      "Loss_val: 641581.5625\n",
      "\n",
      "Эпоха: 543 Loss_train: 265486.19161739864, 0:00:00.096205 сек\n",
      "Loss_val: 640906.625\n",
      "\n",
      "Эпоха: 544 Loss_train: 264911.17683699325, 0:00:00.085211 сек\n",
      "Loss_val: 639964.8125\n",
      "\n",
      "Эпоха: 545 Loss_train: 265125.91311233107, 0:00:00.091076 сек\n",
      "Loss_val: 639533.9375\n",
      "\n",
      "Эпоха: 546 Loss_train: 264850.6550886824, 0:00:00.083032 сек\n",
      "Loss_val: 638551.3125\n",
      "\n",
      "Эпоха: 547 Loss_train: 265239.48263302364, 0:00:00.092177 сек\n",
      "Loss_val: 637657.0\n",
      "\n",
      "Эпоха: 548 Loss_train: 265468.7693728885, 0:00:00.090514 сек\n",
      "Loss_val: 637477.5\n",
      "\n",
      "Эпоха: 549 Loss_train: 264827.09309016046, 0:00:00.100000 сек\n",
      "Loss_val: 637064.0625\n",
      "\n",
      "Эпоха: 550 Loss_train: 264844.78172508447, 0:00:00.083437 сек\n",
      "Loss_val: 636024.0625\n",
      "\n",
      "Эпоха: 551 Loss_train: 265552.2946579392, 0:00:00.103245 сек\n",
      "Loss_val: 635159.3125\n",
      "\n",
      "Эпоха: 552 Loss_train: 265071.21843327704, 0:00:00.101567 сек\n",
      "Loss_val: 634751.5625\n",
      "\n",
      "Эпоха: 553 Loss_train: 264445.5215899493, 0:00:00.096552 сек\n",
      "Loss_val: 633896.625\n",
      "\n",
      "Эпоха: 554 Loss_train: 264583.5904771959, 0:00:00.093073 сек\n",
      "Loss_val: 633422.3125\n",
      "\n",
      "Эпоха: 555 Loss_train: 265295.4328547297, 0:00:00.089698 сек\n",
      "Loss_val: 632796.8125\n",
      "\n",
      "Эпоха: 556 Loss_train: 264201.32918074325, 0:00:00.115077 сек\n",
      "Loss_val: 631711.0625\n",
      "\n",
      "Эпоха: 557 Loss_train: 264274.77306798985, 0:00:00.099992 сек\n",
      "Loss_val: 631251.6875\n",
      "\n",
      "Эпоха: 558 Loss_train: 264331.2160050676, 0:00:00.088700 сек\n",
      "Loss_val: 630762.3125\n",
      "\n",
      "Эпоха: 559 Loss_train: 264172.4258340372, 0:00:00.104324 сек\n",
      "Loss_val: 630171.6875\n",
      "\n",
      "Эпоха: 560 Loss_train: 264242.0565350507, 0:00:00.085175 сек\n",
      "Loss_val: 629198.6875\n",
      "\n",
      "Эпоха: 561 Loss_train: 264045.4478462838, 0:00:00.087030 сек\n",
      "Loss_val: 628590.0\n",
      "\n",
      "Эпоха: 562 Loss_train: 264825.9074641047, 0:00:00.105037 сек\n",
      "Loss_val: 628108.4375\n",
      "\n",
      "Эпоха: 563 Loss_train: 263805.24963048985, 0:00:00.097038 сек\n",
      "Loss_val: 627249.1875\n",
      "\n",
      "Эпоха: 564 Loss_train: 263760.9081503378, 0:00:00.089969 сек\n",
      "Loss_val: 626990.9375\n",
      "\n",
      "Эпоха: 565 Loss_train: 266584.52882179053, 0:00:00.096592 сек\n",
      "Loss_val: 626289.5625\n",
      "\n",
      "Эпоха: 566 Loss_train: 263656.3536211993, 0:00:00.084151 сек\n",
      "Loss_val: 625266.8125\n",
      "\n",
      "Эпоха: 567 Loss_train: 263335.6206714527, 0:00:00.085144 сек\n",
      "Loss_val: 624787.125\n",
      "\n",
      "Эпоха: 568 Loss_train: 263609.84411951015, 0:00:00.088126 сек\n",
      "Loss_val: 624120.4375\n",
      "\n",
      "Эпоха: 569 Loss_train: 263394.0775971284, 0:00:00.098026 сек\n",
      "Loss_val: 623333.9375\n",
      "\n",
      "Эпоха: 570 Loss_train: 263299.9646853885, 0:00:00.077818 сек\n",
      "Loss_val: 623241.4375\n",
      "\n",
      "Эпоха: 571 Loss_train: 263699.5935916385, 0:00:00.086635 сек\n",
      "Loss_val: 622358.0625\n",
      "\n",
      "Эпоха: 572 Loss_train: 263330.3587415541, 0:00:00.087549 сек\n",
      "Loss_val: 621681.0\n",
      "\n",
      "Эпоха: 573 Loss_train: 263179.46492293075, 0:00:00.092605 сек\n",
      "Loss_val: 620905.1875\n",
      "\n",
      "Эпоха: 574 Loss_train: 263220.6688133446, 0:00:00.097254 сек\n",
      "Loss_val: 620564.9375\n",
      "\n",
      "Эпоха: 575 Loss_train: 263009.3214738176, 0:00:00.087107 сек\n",
      "Loss_val: 619913.8125\n",
      "\n",
      "Эпоха: 576 Loss_train: 263341.9656355574, 0:00:00.083617 сек\n",
      "Loss_val: 619672.6875\n",
      "\n",
      "Эпоха: 577 Loss_train: 262781.6125422297, 0:00:00.095128 сек\n",
      "Loss_val: 618961.375\n",
      "\n",
      "Эпоха: 578 Loss_train: 262906.20217483107, 0:00:00.094628 сек\n",
      "Loss_val: 617936.8125\n",
      "\n",
      "Эпоха: 579 Loss_train: 263222.1443728885, 0:00:00.089048 сек\n",
      "Loss_val: 617540.9375\n",
      "\n",
      "Эпоха: 580 Loss_train: 262645.91971072636, 0:00:00.104147 сек\n",
      "Loss_val: 616823.5625\n",
      "\n",
      "Эпоха: 581 Loss_train: 263213.99641047296, 0:00:00.085723 сек\n",
      "Loss_val: 616438.0625\n",
      "\n",
      "Эпоха: 582 Loss_train: 265410.8189928209, 0:00:00.099080 сек\n",
      "Loss_val: 615812.4375\n",
      "\n",
      "Эпоха: 583 Loss_train: 262958.21004011825, 0:00:00.099720 сек\n",
      "Loss_val: 614979.0625\n",
      "\n",
      "Эпоха: 584 Loss_train: 262263.1652766047, 0:00:00.088119 сек\n",
      "Loss_val: 614596.6875\n",
      "\n",
      "Эпоха: 585 Loss_train: 262267.2585515203, 0:00:00.133575 сек\n",
      "Loss_val: 614102.3125\n",
      "\n",
      "Эпоха: 586 Loss_train: 262299.1762035473, 0:00:00.099098 сек\n",
      "Loss_val: 613754.4375\n",
      "\n",
      "Эпоха: 587 Loss_train: 262537.47677364864, 0:00:00.087567 сек\n",
      "Loss_val: 613103.9375\n",
      "\n",
      "Эпоха: 588 Loss_train: 262739.6916701858, 0:00:00.093106 сек\n",
      "Loss_val: 612272.1875\n",
      "\n",
      "Эпоха: 589 Loss_train: 262553.23046875, 0:00:00.086657 сек\n",
      "Loss_val: 612244.375\n",
      "\n",
      "Эпоха: 590 Loss_train: 262105.0680954392, 0:00:00.082685 сек\n",
      "Loss_val: 611194.3125\n",
      "\n",
      "Эпоха: 591 Loss_train: 262478.73015202704, 0:00:00.095617 сек\n",
      "Loss_val: 610717.0\n",
      "\n",
      "Эпоха: 592 Loss_train: 262072.13803842905, 0:00:00.093663 сек\n",
      "Loss_val: 610250.8125\n",
      "\n",
      "Эпоха: 593 Loss_train: 262031.6939400338, 0:00:00.091320 сек\n",
      "Loss_val: 609756.5625\n",
      "\n",
      "Эпоха: 594 Loss_train: 262769.9357052365, 0:00:00.100105 сек\n",
      "Loss_val: 608863.6875\n",
      "\n",
      "Эпоха: 595 Loss_train: 261681.66791596284, 0:00:00.103553 сек\n",
      "Loss_val: 608512.0625\n",
      "\n",
      "Эпоха: 596 Loss_train: 261902.18325063345, 0:00:00.091210 сек\n",
      "Loss_val: 607754.125\n",
      "\n",
      "Эпоха: 597 Loss_train: 261609.46157094595, 0:00:00.108606 сек\n",
      "Loss_val: 607334.5\n",
      "\n",
      "Эпоха: 598 Loss_train: 261643.3424831081, 0:00:00.097158 сек\n",
      "Loss_val: 606840.8125\n",
      "\n",
      "Эпоха: 599 Loss_train: 261761.90023226352, 0:00:00.098128 сек\n",
      "Loss_val: 606272.5625\n",
      "\n",
      "Эпоха: 600 Loss_train: 262403.7103040541, 0:00:00.110522 сек\n",
      "Loss_val: 606258.1875\n",
      "\n",
      "Эпоха: 601 Loss_train: 261629.92240287163, 0:00:00.086623 сек\n",
      "Loss_val: 605343.3125\n",
      "\n",
      "Эпоха: 602 Loss_train: 262061.79608319257, 0:00:00.095151 сек\n",
      "Loss_val: 604782.75\n",
      "\n",
      "Эпоха: 603 Loss_train: 261518.1417335304, 0:00:00.090753 сек\n",
      "Loss_val: 604353.9375\n",
      "\n",
      "Эпоха: 604 Loss_train: 261131.99398226352, 0:00:00.103800 сек\n",
      "Loss_val: 603711.5625\n",
      "\n",
      "Эпоха: 605 Loss_train: 261190.94172297296, 0:00:00.086076 сек\n",
      "Loss_val: 603087.0625\n",
      "\n",
      "Эпоха: 606 Loss_train: 261408.1332347973, 0:00:00.087572 сек\n",
      "Loss_val: 602332.3125\n",
      "\n",
      "Эпоха: 607 Loss_train: 261058.29597761825, 0:00:00.094079 сек\n",
      "Loss_val: 602356.5\n",
      "\n",
      "Эпоха: 608 Loss_train: 261053.69499577704, 0:00:00.096573 сек\n",
      "Loss_val: 601658.125\n",
      "\n",
      "Эпоха: 609 Loss_train: 261526.859375, 0:00:00.089184 сек\n",
      "Loss_val: 601408.5\n",
      "\n",
      "Эпоха: 610 Loss_train: 260931.89273648648, 0:00:00.087119 сек\n",
      "Loss_val: 600895.6875\n",
      "\n",
      "Эпоха: 611 Loss_train: 260793.73474451015, 0:00:00.101574 сек\n",
      "Loss_val: 600231.6875\n",
      "\n",
      "Эпоха: 612 Loss_train: 261566.28272804053, 0:00:00.094141 сек\n",
      "Loss_val: 599924.8125\n",
      "\n",
      "Эпоха: 613 Loss_train: 261408.3073268581, 0:00:00.094122 сек\n",
      "Loss_val: 599405.5625\n",
      "\n",
      "Эпоха: 614 Loss_train: 260699.1432643581, 0:00:00.102624 сек\n",
      "Loss_val: 598442.6875\n",
      "\n",
      "Эпоха: 615 Loss_train: 261222.28425886825, 0:00:00.101623 сек\n",
      "Loss_val: 598410.9375\n",
      "\n",
      "Эпоха: 616 Loss_train: 260970.86694995777, 0:00:00.096072 сек\n",
      "Loss_val: 597998.1875\n",
      "\n",
      "Эпоха: 617 Loss_train: 260838.79830025337, 0:00:00.098711 сек\n",
      "Loss_val: 597572.8125\n",
      "\n",
      "Эпоха: 618 Loss_train: 260721.4104201858, 0:00:00.122273 сек\n",
      "Loss_val: 597058.3125\n",
      "\n",
      "Эпоха: 619 Loss_train: 260820.18327702704, 0:00:00.112759 сек\n",
      "Loss_val: 596623.3125\n",
      "\n",
      "Эпоха: 620 Loss_train: 260830.0614970439, 0:00:00.088581 сек\n",
      "Loss_val: 596137.875\n",
      "\n",
      "Эпоха: 621 Loss_train: 260592.4030299831, 0:00:00.085641 сек\n",
      "Loss_val: 595345.5625\n",
      "\n",
      "Эпоха: 622 Loss_train: 260783.86460092905, 0:00:00.092691 сек\n",
      "Loss_val: 595174.6875\n",
      "\n",
      "Эпоха: 623 Loss_train: 260359.01219383447, 0:00:00.089548 сек\n",
      "Loss_val: 594700.125\n",
      "\n",
      "Эпоха: 624 Loss_train: 260211.3776393581, 0:00:00.097092 сек\n",
      "Loss_val: 594018.6875\n",
      "\n",
      "Эпоха: 625 Loss_train: 260815.3337204392, 0:00:00.079083 сек\n",
      "Loss_val: 593777.1875\n",
      "\n",
      "Эпоха: 626 Loss_train: 260085.58366765204, 0:00:00.087609 сек\n",
      "Loss_val: 593194.4375\n",
      "\n",
      "Эпоха: 627 Loss_train: 260109.42461993243, 0:00:00.095115 сек\n",
      "Loss_val: 592606.25\n",
      "\n",
      "Эпоха: 628 Loss_train: 260080.39722339527, 0:00:00.094705 сек\n",
      "Loss_val: 592357.5625\n",
      "\n",
      "Эпоха: 629 Loss_train: 260082.16137035473, 0:00:00.095231 сек\n",
      "Loss_val: 591953.4375\n",
      "\n",
      "Эпоха: 630 Loss_train: 260042.21684966216, 0:00:00.089567 сек\n",
      "Loss_val: 591203.1875\n",
      "\n",
      "Эпоха: 631 Loss_train: 262060.92372255068, 0:00:00.096109 сек\n",
      "Loss_val: 591020.5\n",
      "\n",
      "Эпоха: 632 Loss_train: 260048.42630912163, 0:00:00.103335 сек\n",
      "Loss_val: 590440.4375\n",
      "\n",
      "Эпоха: 633 Loss_train: 260041.44774070947, 0:00:00.099514 сек\n",
      "Loss_val: 589996.1875\n",
      "\n",
      "Эпоха: 634 Loss_train: 260518.90102407095, 0:00:00.099068 сек\n",
      "Loss_val: 589600.0625\n",
      "\n",
      "Эпоха: 635 Loss_train: 260081.56429476352, 0:00:00.091124 сек\n",
      "Loss_val: 589198.1875\n",
      "\n",
      "Эпоха: 636 Loss_train: 259664.25496199325, 0:00:00.089035 сек\n",
      "Loss_val: 588810.5625\n",
      "\n",
      "Эпоха: 637 Loss_train: 260432.39936127534, 0:00:00.088157 сек\n",
      "Loss_val: 588305.9375\n",
      "\n",
      "Эпоха: 638 Loss_train: 260823.05500422296, 0:00:00.106093 сек\n",
      "Loss_val: 587944.8125\n",
      "\n",
      "Эпоха: 639 Loss_train: 259606.13666596284, 0:00:00.099297 сек\n",
      "Loss_val: 587383.0625\n",
      "\n",
      "Эпоха: 640 Loss_train: 259801.52027027027, 0:00:00.104648 сек\n",
      "Loss_val: 586762.375\n",
      "\n",
      "Эпоха: 641 Loss_train: 259840.3277027027, 0:00:00.110257 сек\n",
      "Loss_val: 586622.6875\n",
      "\n",
      "Эпоха: 642 Loss_train: 259506.12674197636, 0:00:00.103992 сек\n",
      "Loss_val: 586208.5625\n",
      "\n",
      "Эпоха: 643 Loss_train: 259398.02979835303, 0:00:00.097284 сек\n",
      "Loss_val: 585627.5\n",
      "\n",
      "Эпоха: 644 Loss_train: 259806.4585752745, 0:00:00.111605 сек\n",
      "Loss_val: 585501.5625\n",
      "\n",
      "Эпоха: 645 Loss_train: 259644.8704075169, 0:00:00.110580 сек\n",
      "Loss_val: 585119.3125\n",
      "\n",
      "Эпоха: 646 Loss_train: 259789.12711148648, 0:00:00.100128 сек\n",
      "Loss_val: 584527.0\n",
      "\n",
      "Эпоха: 647 Loss_train: 259285.51942567568, 0:00:00.096398 сек\n",
      "Loss_val: 584156.375\n",
      "\n",
      "Эпоха: 648 Loss_train: 259286.0178420608, 0:00:00.118308 сек\n",
      "Loss_val: 583537.5625\n",
      "\n",
      "Эпоха: 649 Loss_train: 259754.7525073902, 0:00:00.111070 сек\n",
      "Loss_val: 583165.875\n",
      "\n",
      "Эпоха: 650 Loss_train: 259428.5772804054, 0:00:00.098673 сек\n",
      "Loss_val: 582756.1875\n",
      "\n",
      "Эпоха: 651 Loss_train: 259089.98796452704, 0:00:00.104076 сек\n",
      "Loss_val: 582555.0\n",
      "\n",
      "Эпоха: 652 Loss_train: 259930.19917915962, 0:00:00.091502 сек\n",
      "Loss_val: 582343.1875\n",
      "\n",
      "Эпоха: 653 Loss_train: 260328.9448902027, 0:00:00.094832 сек\n",
      "Loss_val: 581848.0625\n",
      "\n",
      "Эпоха: 654 Loss_train: 259649.6593116554, 0:00:00.121175 сек\n",
      "Loss_val: 581284.0625\n",
      "\n",
      "Эпоха: 655 Loss_train: 259129.3395798142, 0:00:00.102138 сек\n",
      "Loss_val: 580645.25\n",
      "\n",
      "Эпоха: 656 Loss_train: 259025.77787162163, 0:00:00.091324 сек\n",
      "Loss_val: 580539.125\n",
      "\n",
      "Эпоха: 657 Loss_train: 259638.4658467061, 0:00:00.106382 сек\n",
      "Loss_val: 580295.125\n",
      "\n",
      "Эпоха: 658 Loss_train: 259288.4557643581, 0:00:00.099620 сек\n",
      "Loss_val: 579671.625\n",
      "\n",
      "Эпоха: 659 Loss_train: 259714.5242820946, 0:00:00.094503 сек\n",
      "Loss_val: 579172.875\n",
      "\n",
      "Эпоха: 660 Loss_train: 259049.1082664696, 0:00:00.106070 сек\n",
      "Loss_val: 578709.5\n",
      "\n",
      "Эпоха: 661 Loss_train: 258989.7802998311, 0:00:00.083087 сек\n",
      "Loss_val: 578709.9375\n",
      "\n",
      "Эпоха: 662 Loss_train: 258945.80004222973, 0:00:00.110158 сек\n",
      "Loss_val: 578172.1875\n",
      "\n",
      "Эпоха: 663 Loss_train: 258740.3314505912, 0:00:00.100761 сек\n",
      "Loss_val: 577689.4375\n",
      "\n",
      "Эпоха: 664 Loss_train: 259252.62996199325, 0:00:00.110529 сек\n",
      "Loss_val: 577250.4375\n",
      "\n",
      "Эпоха: 665 Loss_train: 258758.44467905405, 0:00:00.102525 сек\n",
      "Loss_val: 577011.5625\n",
      "\n",
      "Эпоха: 666 Loss_train: 259070.21890836148, 0:00:00.092427 сек\n",
      "Loss_val: 576790.6875\n",
      "\n",
      "Эпоха: 667 Loss_train: 258576.7801414696, 0:00:00.092687 сек\n",
      "Loss_val: 576437.875\n",
      "\n",
      "Эпоха: 668 Loss_train: 258434.11235747466, 0:00:00.104298 сек\n",
      "Loss_val: 576193.0625\n",
      "\n",
      "Эпоха: 669 Loss_train: 258686.8918918919, 0:00:00.094257 сек\n",
      "Loss_val: 575755.1875\n",
      "\n",
      "Эпоха: 670 Loss_train: 258954.29447318413, 0:00:00.087257 сек\n",
      "Loss_val: 575263.6875\n",
      "\n",
      "Эпоха: 671 Loss_train: 258549.42768158784, 0:00:00.099407 сек\n",
      "Loss_val: 575038.125\n",
      "\n",
      "Эпоха: 672 Loss_train: 258336.0364231419, 0:00:00.096578 сек\n",
      "Loss_val: 574645.625\n",
      "\n",
      "Эпоха: 673 Loss_train: 258774.69805743243, 0:00:00.104635 сек\n",
      "Loss_val: 574521.625\n",
      "\n",
      "Эпоха: 674 Loss_train: 258139.44898120777, 0:00:00.112836 сек\n",
      "Loss_val: 574085.5625\n",
      "\n",
      "Эпоха: 675 Loss_train: 259059.54054054053, 0:00:00.092854 сек\n",
      "Loss_val: 573489.0625\n",
      "\n",
      "Эпоха: 676 Loss_train: 258223.26177153716, 0:00:00.111275 сек\n",
      "Loss_val: 573242.875\n",
      "\n",
      "Эпоха: 677 Loss_train: 259079.94842694257, 0:00:00.092652 сек\n",
      "Loss_val: 572923.5625\n",
      "\n",
      "Эпоха: 678 Loss_train: 258162.60628695102, 0:00:00.101170 сек\n",
      "Loss_val: 572585.375\n",
      "\n",
      "Эпоха: 679 Loss_train: 258264.26013513515, 0:00:00.096768 сек\n",
      "Loss_val: 572098.9375\n",
      "\n",
      "Эпоха: 680 Loss_train: 258163.8741554054, 0:00:00.084189 сек\n",
      "Loss_val: 571785.0\n",
      "\n",
      "Эпоха: 681 Loss_train: 258321.89936127534, 0:00:00.090045 сек\n",
      "Loss_val: 571411.5625\n",
      "\n",
      "Эпоха: 682 Loss_train: 258292.72212837837, 0:00:00.104562 сек\n",
      "Loss_val: 570890.875\n",
      "\n",
      "Эпоха: 683 Loss_train: 258712.87800886825, 0:00:00.091098 сек\n",
      "Loss_val: 570894.125\n",
      "\n",
      "Эпоха: 684 Loss_train: 258001.4817356419, 0:00:00.090507 сек\n",
      "Loss_val: 570243.8125\n",
      "\n",
      "Эпоха: 685 Loss_train: 258158.9944045608, 0:00:00.111674 сек\n",
      "Loss_val: 569865.4375\n",
      "\n",
      "Эпоха: 686 Loss_train: 258243.91327069257, 0:00:00.106214 сек\n",
      "Loss_val: 569930.75\n",
      "\n",
      "Эпоха: 687 Loss_train: 257787.4949852196, 0:00:00.090134 сек\n",
      "Loss_val: 569477.25\n",
      "\n",
      "Эпоха: 688 Loss_train: 257790.2204391892, 0:00:00.092271 сек\n",
      "Loss_val: 569113.9375\n",
      "\n",
      "Эпоха: 689 Loss_train: 257899.7131545608, 0:00:00.089778 сек\n",
      "Loss_val: 568849.4375\n",
      "\n",
      "Эпоха: 690 Loss_train: 258125.6831714527, 0:00:00.114677 сек\n",
      "Loss_val: 568383.4375\n",
      "\n",
      "Эпоха: 691 Loss_train: 257778.1655933277, 0:00:00.083319 сек\n",
      "Loss_val: 567892.9375\n",
      "\n",
      "Эпоха: 692 Loss_train: 259123.64806798985, 0:00:00.110182 сек\n",
      "Loss_val: 567722.25\n",
      "\n",
      "Эпоха: 693 Loss_train: 258761.70951224663, 0:00:00.090728 сек\n",
      "Loss_val: 567222.25\n",
      "\n",
      "Эпоха: 694 Loss_train: 257817.37711148648, 0:00:00.097113 сек\n",
      "Loss_val: 567038.75\n",
      "\n",
      "Эпоха: 695 Loss_train: 258321.22735430743, 0:00:00.094172 сек\n",
      "Loss_val: 566822.75\n",
      "\n",
      "Эпоха: 696 Loss_train: 257815.5262352196, 0:00:00.112594 сек\n",
      "Loss_val: 566440.5625\n",
      "\n",
      "Эпоха: 697 Loss_train: 258152.07928631757, 0:00:00.090642 сек\n",
      "Loss_val: 565999.3125\n",
      "\n",
      "Эпоха: 698 Loss_train: 257524.0693095439, 0:00:00.090123 сек\n",
      "Loss_val: 565531.6875\n",
      "\n",
      "Эпоха: 699 Loss_train: 257817.5986064189, 0:00:00.095098 сек\n",
      "Loss_val: 565532.375\n",
      "\n",
      "Эпоха: 700 Loss_train: 257592.947265625, 0:00:00.080877 сек\n",
      "Loss_val: 565080.5625\n",
      "\n",
      "Эпоха: 701 Loss_train: 257580.34285261825, 0:00:00.087087 сек\n",
      "Loss_val: 564813.125\n",
      "\n",
      "Эпоха: 702 Loss_train: 257327.0878642314, 0:00:00.096179 сек\n",
      "Loss_val: 564516.25\n",
      "\n",
      "Эпоха: 703 Loss_train: 257640.8947951858, 0:00:00.101792 сек\n",
      "Loss_val: 564278.5625\n",
      "\n",
      "Эпоха: 704 Loss_train: 257620.90672508447, 0:00:00.101177 сек\n",
      "Loss_val: 563849.6875\n",
      "\n",
      "Эпоха: 705 Loss_train: 258097.7165329392, 0:00:00.108733 сек\n",
      "Loss_val: 563713.75\n",
      "\n",
      "Эпоха: 706 Loss_train: 257282.9563450169, 0:00:00.110241 сек\n",
      "Loss_val: 563355.1875\n",
      "\n",
      "Эпоха: 707 Loss_train: 257158.17863175675, 0:00:00.097195 сек\n",
      "Loss_val: 563062.5\n",
      "\n",
      "Эпоха: 708 Loss_train: 257532.47640413852, 0:00:00.094110 сек\n",
      "Loss_val: 562917.5\n",
      "\n",
      "Эпоха: 709 Loss_train: 257929.48421663852, 0:00:00.102721 сек\n",
      "Loss_val: 562755.4375\n",
      "\n",
      "Эпоха: 710 Loss_train: 257250.56577280405, 0:00:00.096208 сек\n",
      "Loss_val: 562159.75\n",
      "\n",
      "Эпоха: 711 Loss_train: 257191.47888513515, 0:00:00.100091 сек\n",
      "Loss_val: 562111.3125\n",
      "\n",
      "Эпоха: 712 Loss_train: 257326.86929898648, 0:00:00.110546 сек\n",
      "Loss_val: 561428.375\n",
      "\n",
      "Эпоха: 713 Loss_train: 257278.99656883447, 0:00:00.094727 сек\n",
      "Loss_val: 561342.625\n",
      "\n",
      "Эпоха: 714 Loss_train: 257031.44779349663, 0:00:00.094692 сек\n",
      "Loss_val: 560883.375\n",
      "\n",
      "Эпоха: 715 Loss_train: 257662.08562077704, 0:00:00.085702 сек\n",
      "Loss_val: 560854.6875\n",
      "\n",
      "Эпоха: 716 Loss_train: 257798.96669130068, 0:00:00.106320 сек\n",
      "Loss_val: 560267.625\n",
      "\n",
      "Эпоха: 717 Loss_train: 256907.26319679053, 0:00:00.083048 сек\n",
      "Loss_val: 560042.0625\n",
      "\n",
      "Эпоха: 718 Loss_train: 256974.64051942568, 0:00:00.095595 сек\n",
      "Loss_val: 559809.5625\n",
      "\n",
      "Эпоха: 719 Loss_train: 257343.49625211148, 0:00:00.101283 сек\n",
      "Loss_val: 559366.25\n",
      "\n",
      "Эпоха: 720 Loss_train: 257418.4407200169, 0:00:00.096117 сек\n",
      "Loss_val: 559018.0625\n",
      "\n",
      "Эпоха: 721 Loss_train: 256710.43480785473, 0:00:00.097734 сек\n",
      "Loss_val: 558900.0625\n",
      "\n",
      "Эпоха: 722 Loss_train: 257165.15836148648, 0:00:00.097067 сек\n",
      "Loss_val: 558454.5\n",
      "\n",
      "Эпоха: 723 Loss_train: 256920.73601140204, 0:00:00.099575 сек\n",
      "Loss_val: 558396.4375\n",
      "\n",
      "Эпоха: 724 Loss_train: 256979.41426045186, 0:00:00.082099 сек\n",
      "Loss_val: 557918.125\n",
      "\n",
      "Эпоха: 725 Loss_train: 257201.5269214527, 0:00:00.103489 сек\n",
      "Loss_val: 557687.8125\n",
      "\n",
      "Эпоха: 726 Loss_train: 256658.5715793919, 0:00:00.138593 сек\n",
      "Loss_val: 557777.9375\n",
      "\n",
      "Эпоха: 727 Loss_train: 256583.58583192568, 0:00:00.092255 сек\n",
      "Loss_val: 557219.1875\n",
      "\n",
      "Эпоха: 728 Loss_train: 256772.7591849662, 0:00:00.093564 сек\n",
      "Loss_val: 556848.4375\n",
      "\n",
      "Эпоха: 729 Loss_train: 256551.21516047296, 0:00:00.088539 сек\n",
      "Loss_val: 556578.125\n",
      "\n",
      "Эпоха: 730 Loss_train: 256783.0780986064, 0:00:00.090126 сек\n",
      "Loss_val: 556440.6875\n",
      "\n",
      "Эпоха: 731 Loss_train: 257240.2578125, 0:00:00.092042 сек\n",
      "Loss_val: 556383.9375\n",
      "\n",
      "Эпоха: 732 Loss_train: 257037.0104782517, 0:00:00.088042 сек\n",
      "Loss_val: 556038.4375\n",
      "\n",
      "Эпоха: 733 Loss_train: 256514.44737119932, 0:00:00.131923 сек\n",
      "Loss_val: 555590.25\n",
      "\n",
      "Эпоха: 734 Loss_train: 256647.99044552364, 0:00:00.120263 сек\n",
      "Loss_val: 555364.1875\n",
      "\n",
      "Эпоха: 735 Loss_train: 256540.28373099663, 0:00:00.110512 сек\n",
      "Loss_val: 555271.3125\n",
      "\n",
      "Эпоха: 736 Loss_train: 259447.20835092905, 0:00:00.093161 сек\n",
      "Loss_val: 555286.6875\n",
      "\n",
      "Эпоха: 737 Loss_train: 256255.3704075169, 0:00:00.083213 сек\n",
      "Loss_val: 554742.1875\n",
      "\n",
      "Эпоха: 738 Loss_train: 256377.5534206081, 0:00:00.098161 сек\n",
      "Loss_val: 554420.8125\n",
      "\n",
      "Эпоха: 739 Loss_train: 256945.22856841216, 0:00:00.084082 сек\n",
      "Loss_val: 554177.375\n",
      "\n",
      "Эпоха: 740 Loss_train: 257089.13492398648, 0:00:00.078031 сек\n",
      "Loss_val: 553620.8125\n",
      "\n",
      "Эпоха: 741 Loss_train: 256342.96996410473, 0:00:00.093567 сек\n",
      "Loss_val: 553546.3125\n",
      "\n",
      "Эпоха: 742 Loss_train: 256364.7371727196, 0:00:00.086078 сек\n",
      "Loss_val: 553575.1875\n",
      "\n",
      "Эпоха: 743 Loss_train: 256838.34142736485, 0:00:00.084580 сек\n",
      "Loss_val: 553194.0625\n",
      "\n",
      "Эпоха: 744 Loss_train: 256107.2070048564, 0:00:00.093073 сек\n",
      "Loss_val: 552838.25\n",
      "\n",
      "Эпоха: 745 Loss_train: 256626.04819467905, 0:00:00.082026 сек\n",
      "Loss_val: 552721.375\n",
      "\n",
      "Эпоха: 746 Loss_train: 256289.98764780405, 0:00:00.088605 сек\n",
      "Loss_val: 552220.9375\n",
      "\n",
      "Эпоха: 747 Loss_train: 256801.97735430743, 0:00:00.104059 сек\n",
      "Loss_val: 551973.375\n",
      "\n",
      "Эпоха: 748 Loss_train: 256317.80109797296, 0:00:00.099625 сек\n",
      "Loss_val: 551977.9375\n",
      "\n",
      "Эпоха: 749 Loss_train: 259198.79544974663, 0:00:00.094142 сек\n",
      "Loss_val: 551678.1875\n",
      "\n",
      "Эпоха: 750 Loss_train: 257004.88012035473, 0:00:00.096664 сек\n",
      "Loss_val: 551314.0\n",
      "\n",
      "Эпоха: 751 Loss_train: 255997.0591744088, 0:00:00.095247 сек\n",
      "Loss_val: 551129.375\n",
      "\n",
      "Эпоха: 752 Loss_train: 256855.56297508447, 0:00:00.101623 сек\n",
      "Loss_val: 550833.125\n",
      "\n",
      "Эпоха: 753 Loss_train: 256029.26082136825, 0:00:00.087061 сек\n",
      "Loss_val: 550776.5625\n",
      "\n",
      "Эпоха: 754 Loss_train: 256166.06714527027, 0:00:00.081550 сек\n",
      "Loss_val: 550596.6875\n",
      "\n",
      "Эпоха: 755 Loss_train: 257556.22482052364, 0:00:00.094103 сек\n",
      "Loss_val: 550194.75\n",
      "\n",
      "Эпоха: 756 Loss_train: 256778.67279877534, 0:00:00.096124 сек\n",
      "Loss_val: 550095.875\n",
      "\n",
      "Эпоха: 757 Loss_train: 256401.80806587837, 0:00:00.095132 сек\n",
      "Loss_val: 549549.125\n",
      "\n",
      "Эпоха: 758 Loss_train: 255774.6304370777, 0:00:00.098179 сек\n",
      "Loss_val: 549838.5625\n",
      "\n",
      "Эпоха: 759 Loss_train: 256237.96040962837, 0:00:00.092036 сек\n",
      "Loss_val: 549208.0625\n",
      "\n",
      "Эпоха: 760 Loss_train: 255993.45539484796, 0:00:00.095114 сек\n",
      "Loss_val: 549102.9375\n",
      "\n",
      "Эпоха: 761 Loss_train: 256380.1428684544, 0:00:00.101106 сек\n",
      "Loss_val: 549055.5625\n",
      "\n",
      "Эпоха: 762 Loss_train: 256142.90398015204, 0:00:00.103504 сек\n",
      "Loss_val: 548372.125\n",
      "\n",
      "Эпоха: 763 Loss_train: 255911.82316300675, 0:00:00.089038 сек\n",
      "Loss_val: 548403.6875\n",
      "\n",
      "Эпоха: 764 Loss_train: 256469.79117398648, 0:00:00.098077 сек\n",
      "Loss_val: 548267.6875\n",
      "\n",
      "Эпоха: 765 Loss_train: 255697.5972339527, 0:00:00.099128 сек\n",
      "Loss_val: 547995.8125\n",
      "\n",
      "Эпоха: 766 Loss_train: 255788.82068201015, 0:00:00.083189 сек\n",
      "Loss_val: 547412.0625\n",
      "\n",
      "Эпоха: 767 Loss_train: 256015.2079814189, 0:00:00.101650 сек\n",
      "Loss_val: 547364.0625\n",
      "\n",
      "Эпоха: 768 Loss_train: 256793.23210515204, 0:00:00.096607 сек\n",
      "Loss_val: 547252.1875\n",
      "\n",
      "Эпоха: 769 Loss_train: 256361.54719172296, 0:00:00.099140 сек\n",
      "Loss_val: 546692.1875\n",
      "\n",
      "Эпоха: 770 Loss_train: 256142.28726773648, 0:00:00.094057 сек\n",
      "Loss_val: 546852.1875\n",
      "\n",
      "Эпоха: 771 Loss_train: 255717.69832136825, 0:00:00.096032 сек\n",
      "Loss_val: 546343.75\n",
      "\n",
      "Эпоха: 772 Loss_train: 255392.82706925675, 0:00:00.089038 сек\n",
      "Loss_val: 545984.0\n",
      "\n",
      "Эпоха: 773 Loss_train: 255386.05822423985, 0:00:00.105613 сек\n",
      "Loss_val: 546063.3125\n",
      "\n",
      "Эпоха: 774 Loss_train: 256529.2451435811, 0:00:00.090668 сек\n",
      "Loss_val: 545737.8125\n",
      "\n",
      "Эпоха: 775 Loss_train: 255549.33514569257, 0:00:00.080564 сек\n",
      "Loss_val: 545564.1875\n",
      "\n",
      "Эпоха: 776 Loss_train: 255210.6376689189, 0:00:00.114017 сек\n",
      "Loss_val: 545358.6875\n",
      "\n",
      "Эпоха: 777 Loss_train: 255692.8055320946, 0:00:00.089995 сек\n",
      "Loss_val: 544931.875\n",
      "\n",
      "Эпоха: 778 Loss_train: 255352.8512985642, 0:00:00.091071 сек\n",
      "Loss_val: 544904.9375\n",
      "\n",
      "Эпоха: 779 Loss_train: 255706.1565139358, 0:00:00.092214 сек\n",
      "Loss_val: 544838.0625\n",
      "\n",
      "Эпоха: 780 Loss_train: 258437.99688555743, 0:00:00.083133 сек\n",
      "Loss_val: 544623.4375\n",
      "\n",
      "Эпоха: 781 Loss_train: 255307.7600295608, 0:00:00.094386 сек\n",
      "Loss_val: 544495.8125\n",
      "\n",
      "Эпоха: 782 Loss_train: 255377.65767525337, 0:00:00.101270 сек\n",
      "Loss_val: 544023.0\n",
      "\n",
      "Эпоха: 783 Loss_train: 255927.76496516046, 0:00:00.095076 сек\n",
      "Loss_val: 543864.375\n",
      "\n",
      "Эпоха: 784 Loss_train: 255468.2395481419, 0:00:00.082079 сек\n",
      "Loss_val: 543664.5625\n",
      "\n",
      "Эпоха: 785 Loss_train: 255363.65215899493, 0:00:00.086262 сек\n",
      "Loss_val: 543525.4375\n",
      "\n",
      "Эпоха: 786 Loss_train: 255022.23585304053, 0:00:00.100000 сек\n",
      "Loss_val: 543386.875\n",
      "\n",
      "Эпоха: 787 Loss_train: 255181.1858108108, 0:00:00.103523 сек\n",
      "Loss_val: 543420.0625\n",
      "\n",
      "Эпоха: 788 Loss_train: 255886.60620777027, 0:00:00.101070 сек\n",
      "Loss_val: 542728.25\n",
      "\n",
      "Эпоха: 789 Loss_train: 255148.79676942568, 0:00:00.087735 сек\n",
      "Loss_val: 542951.4375\n",
      "\n",
      "Эпоха: 790 Loss_train: 255112.84968855575, 0:00:00.092615 сек\n",
      "Loss_val: 542442.5625\n",
      "\n",
      "Эпоха: 791 Loss_train: 255159.20249155405, 0:00:00.096758 сек\n",
      "Loss_val: 542350.875\n",
      "\n",
      "Эпоха: 792 Loss_train: 255245.66026182432, 0:00:00.083294 сек\n",
      "Loss_val: 541891.3125\n",
      "\n",
      "Эпоха: 793 Loss_train: 255572.26485958614, 0:00:00.100166 сек\n",
      "Loss_val: 541867.8125\n",
      "\n",
      "Эпоха: 794 Loss_train: 256735.64252533784, 0:00:00.096593 сек\n",
      "Loss_val: 541775.625\n",
      "\n",
      "Эпоха: 795 Loss_train: 255508.7119404561, 0:00:00.088468 сек\n",
      "Loss_val: 541570.0625\n",
      "\n",
      "Эпоха: 796 Loss_train: 255676.15867820947, 0:00:00.103338 сек\n",
      "Loss_val: 541082.8125\n",
      "\n",
      "Эпоха: 797 Loss_train: 255314.97592905405, 0:00:00.091063 сек\n",
      "Loss_val: 540927.125\n",
      "\n",
      "Эпоха: 798 Loss_train: 254965.9927681588, 0:00:00.092228 сек\n",
      "Loss_val: 541119.4375\n",
      "\n",
      "Эпоха: 799 Loss_train: 255377.37299408784, 0:00:00.118614 сек\n",
      "Loss_val: 540861.875\n",
      "\n",
      "Эпоха: 800 Loss_train: 254829.57474662163, 0:00:00.088161 сек\n",
      "Loss_val: 540546.1875\n",
      "\n",
      "Эпоха: 801 Loss_train: 255047.24910261825, 0:00:00.096215 сек\n",
      "Loss_val: 540284.1875\n",
      "\n",
      "Эпоха: 802 Loss_train: 255124.78441722973, 0:00:00.112113 сек\n",
      "Loss_val: 540220.0625\n",
      "\n",
      "Эпоха: 803 Loss_train: 255021.5726879223, 0:00:00.089144 сек\n",
      "Loss_val: 539771.3125\n",
      "\n",
      "Эпоха: 804 Loss_train: 254870.19172297296, 0:00:00.080060 сек\n",
      "Loss_val: 539679.9375\n",
      "\n",
      "Эпоха: 805 Loss_train: 255382.7074535473, 0:00:00.094552 сек\n",
      "Loss_val: 539801.4375\n",
      "\n",
      "Эпоха: 806 Loss_train: 254980.34786739864, 0:00:00.096168 сек\n",
      "Loss_val: 539308.8125\n",
      "\n",
      "Эпоха: 807 Loss_train: 255206.4970967061, 0:00:00.086639 сек\n",
      "Loss_val: 539309.4375\n",
      "\n",
      "Эпоха: 808 Loss_train: 254899.89859586148, 0:00:00.095053 сек\n",
      "Loss_val: 538879.0\n",
      "\n",
      "Эпоха: 809 Loss_train: 254653.8738914696, 0:00:00.085582 сек\n",
      "Loss_val: 538654.4375\n",
      "\n",
      "Эпоха: 810 Loss_train: 254686.80210092905, 0:00:00.110177 сек\n",
      "Loss_val: 538535.9375\n",
      "\n",
      "Эпоха: 811 Loss_train: 254807.4146431588, 0:00:00.101047 сек\n",
      "Loss_val: 538414.8125\n",
      "\n",
      "Эпоха: 812 Loss_train: 254975.86908783784, 0:00:00.119573 сек\n",
      "Loss_val: 538091.3125\n",
      "\n",
      "Эпоха: 813 Loss_train: 254735.9775126689, 0:00:00.110701 сек\n",
      "Loss_val: 538372.5625\n",
      "\n",
      "Эпоха: 814 Loss_train: 254890.5862014358, 0:00:00.090516 сек\n",
      "Loss_val: 537951.4375\n",
      "\n",
      "Эпоха: 815 Loss_train: 254798.2699271537, 0:00:00.092089 сек\n",
      "Loss_val: 537621.1875\n",
      "\n",
      "Эпоха: 816 Loss_train: 254569.02306798985, 0:00:00.099074 сек\n",
      "Loss_val: 537631.3125\n",
      "\n",
      "Эпоха: 817 Loss_train: 254664.34422508447, 0:00:00.104089 сек\n",
      "Loss_val: 537594.5625\n",
      "\n",
      "Эпоха: 818 Loss_train: 254278.56841216216, 0:00:00.088600 сек\n",
      "Loss_val: 537425.5625\n",
      "\n",
      "Эпоха: 819 Loss_train: 254418.1200907939, 0:00:00.084049 сек\n",
      "Loss_val: 537139.625\n",
      "\n",
      "Эпоха: 820 Loss_train: 255741.72387035473, 0:00:00.089003 сек\n",
      "Loss_val: 537140.4375\n",
      "\n",
      "Эпоха: 821 Loss_train: 254758.5619193412, 0:00:00.085346 сек\n",
      "Loss_val: 536657.5625\n",
      "\n",
      "Эпоха: 822 Loss_train: 254392.64044024493, 0:00:00.088656 сек\n",
      "Loss_val: 536607.5625\n",
      "\n",
      "Эпоха: 823 Loss_train: 254352.58044763515, 0:00:00.098141 сек\n",
      "Loss_val: 536538.1875\n",
      "\n",
      "Эпоха: 824 Loss_train: 254643.88492398648, 0:00:00.079129 сек\n",
      "Loss_val: 536134.9375\n",
      "\n",
      "Эпоха: 825 Loss_train: 254661.82194890204, 0:00:00.090177 сек\n",
      "Loss_val: 535937.1875\n",
      "\n",
      "Эпоха: 826 Loss_train: 255473.4507495777, 0:00:00.089581 сек\n",
      "Loss_val: 535897.5625\n",
      "\n",
      "Эпоха: 827 Loss_train: 254431.75593855575, 0:00:00.084134 сек\n",
      "Loss_val: 535725.8125\n",
      "\n",
      "Эпоха: 828 Loss_train: 254155.7241870777, 0:00:00.084661 сек\n",
      "Loss_val: 535329.6875\n",
      "\n",
      "Эпоха: 829 Loss_train: 254049.44597233954, 0:00:00.096002 сек\n",
      "Loss_val: 535474.3125\n",
      "\n",
      "Эпоха: 830 Loss_train: 254194.87299408784, 0:00:00.095438 сек\n",
      "Loss_val: 535258.6875\n",
      "\n",
      "Эпоха: 831 Loss_train: 254334.57442989864, 0:00:00.093880 сек\n",
      "Loss_val: 534901.25\n",
      "\n",
      "Эпоха: 832 Loss_train: 254148.0934860642, 0:00:00.104936 сек\n",
      "Loss_val: 534783.8125\n",
      "\n",
      "Эпоха: 833 Loss_train: 254408.2813819679, 0:00:00.083225 сек\n",
      "Loss_val: 534598.4375\n",
      "\n",
      "Эпоха: 834 Loss_train: 254287.00258657095, 0:00:00.092719 сек\n",
      "Loss_val: 534349.75\n",
      "\n",
      "Эпоха: 835 Loss_train: 253981.1706081081, 0:00:00.086050 сек\n",
      "Loss_val: 534235.625\n",
      "\n",
      "Эпоха: 836 Loss_train: 254309.2061866554, 0:00:00.086109 сек\n",
      "Loss_val: 534096.4375\n",
      "\n",
      "Эпоха: 837 Loss_train: 253955.6209617821, 0:00:00.090583 сек\n",
      "Loss_val: 533930.25\n",
      "\n",
      "Эпоха: 838 Loss_train: 255119.74619932432, 0:00:00.103642 сек\n",
      "Loss_val: 533794.125\n",
      "\n",
      "Эпоха: 839 Loss_train: 254475.61407305743, 0:00:00.091204 сек\n",
      "Loss_val: 533869.5625\n",
      "\n",
      "Эпоха: 840 Loss_train: 254002.18839738175, 0:00:00.080555 сек\n",
      "Loss_val: 533329.6875\n",
      "\n",
      "Эпоха: 841 Loss_train: 254331.36945734796, 0:00:00.087756 сек\n",
      "Loss_val: 533181.1875\n",
      "\n",
      "Эпоха: 842 Loss_train: 254088.25036951015, 0:00:00.095602 сек\n",
      "Loss_val: 533183.5\n",
      "\n",
      "Эпоха: 843 Loss_train: 255088.03536739864, 0:00:00.095553 сек\n",
      "Loss_val: 532930.9375\n",
      "\n",
      "Эпоха: 844 Loss_train: 254291.6312816723, 0:00:00.094378 сек\n",
      "Loss_val: 533029.4375\n",
      "\n",
      "Эпоха: 845 Loss_train: 253866.9715477196, 0:00:00.093930 сек\n",
      "Loss_val: 532671.1875\n",
      "\n",
      "Эпоха: 846 Loss_train: 255147.3567092483, 0:00:00.091137 сек\n",
      "Loss_val: 532683.1875\n",
      "\n",
      "Эпоха: 847 Loss_train: 254220.2276710304, 0:00:00.080543 сек\n",
      "Loss_val: 532491.9375\n",
      "\n",
      "Эпоха: 848 Loss_train: 253906.5612331081, 0:00:00.094595 сек\n",
      "Loss_val: 532257.8125\n",
      "\n",
      "Эпоха: 849 Loss_train: 254316.6866422086, 0:00:00.097187 сек\n",
      "Loss_val: 532119.0625\n",
      "\n",
      "Эпоха: 850 Loss_train: 253558.2589210304, 0:00:00.100698 сек\n",
      "Loss_val: 531885.25\n",
      "\n",
      "Эпоха: 851 Loss_train: 253902.13846072636, 0:00:00.094173 сек\n",
      "Loss_val: 531887.4375\n",
      "\n",
      "Эпоха: 852 Loss_train: 253866.10567989864, 0:00:00.097389 сек\n",
      "Loss_val: 531373.8125\n",
      "\n",
      "Эпоха: 853 Loss_train: 254172.27005912163, 0:00:00.093871 сек\n",
      "Loss_val: 531222.8125\n",
      "\n",
      "Эпоха: 854 Loss_train: 254101.7943412162, 0:00:00.085512 сек\n",
      "Loss_val: 531295.0625\n",
      "\n",
      "Эпоха: 855 Loss_train: 254181.55405405405, 0:00:00.096513 сек\n",
      "Loss_val: 531278.4375\n",
      "\n",
      "Эпоха: 856 Loss_train: 253596.4462626689, 0:00:00.078055 сек\n",
      "Loss_val: 530832.5625\n",
      "\n",
      "Эпоха: 857 Loss_train: 253641.4765625, 0:00:00.080004 сек\n",
      "Loss_val: 530874.25\n",
      "\n",
      "Эпоха: 858 Loss_train: 254029.63236380913, 0:00:00.110525 сек\n",
      "Loss_val: 530552.9375\n",
      "\n",
      "Эпоха: 859 Loss_train: 253963.8899387669, 0:00:00.106100 сек\n",
      "Loss_val: 530595.4375\n",
      "\n",
      "Эпоха: 860 Loss_train: 253703.95386402027, 0:00:00.099551 сек\n",
      "Loss_val: 530509.625\n",
      "\n",
      "Эпоха: 861 Loss_train: 253351.42652027027, 0:00:00.103732 сек\n",
      "Loss_val: 530128.75\n",
      "\n",
      "Эпоха: 862 Loss_train: 253779.5544235642, 0:00:00.081618 сек\n",
      "Loss_val: 530197.625\n",
      "\n",
      "Эпоха: 863 Loss_train: 253648.5179740287, 0:00:00.095096 сек\n",
      "Loss_val: 529893.8125\n",
      "\n",
      "Эпоха: 864 Loss_train: 253561.1601298564, 0:00:00.089548 сек\n",
      "Loss_val: 529796.0625\n",
      "\n",
      "Эпоха: 865 Loss_train: 253707.16205658784, 0:00:00.080603 сек\n",
      "Loss_val: 529745.625\n",
      "\n",
      "Эпоха: 866 Loss_train: 254080.05394847973, 0:00:00.095578 сек\n",
      "Loss_val: 529567.5\n",
      "\n",
      "Эпоха: 867 Loss_train: 253775.56909839527, 0:00:00.091199 сек\n",
      "Loss_val: 529346.0625\n",
      "\n",
      "Эпоха: 868 Loss_train: 254185.6948902027, 0:00:00.091519 сек\n",
      "Loss_val: 529238.625\n",
      "\n",
      "Эпоха: 869 Loss_train: 253591.2565456081, 0:00:00.112114 сек\n",
      "Loss_val: 529062.0\n",
      "\n",
      "Эпоха: 870 Loss_train: 253887.37505278716, 0:00:00.089615 сек\n",
      "Loss_val: 528825.0625\n",
      "\n",
      "Эпоха: 871 Loss_train: 253222.47904349663, 0:00:00.097494 сек\n",
      "Loss_val: 528763.125\n",
      "\n",
      "Эпоха: 872 Loss_train: 253335.66606841216, 0:00:00.089621 сек\n",
      "Loss_val: 528918.25\n",
      "\n",
      "Эпоха: 873 Loss_train: 253307.2861064189, 0:00:00.079537 сек\n",
      "Loss_val: 528544.875\n",
      "\n",
      "Эпоха: 874 Loss_train: 253225.94367609796, 0:00:00.090600 сек\n",
      "Loss_val: 528417.9375\n",
      "\n",
      "Эпоха: 875 Loss_train: 253179.1513671875, 0:00:00.087048 сек\n",
      "Loss_val: 528155.0625\n",
      "\n",
      "Эпоха: 876 Loss_train: 253283.03462837837, 0:00:00.086556 сек\n",
      "Loss_val: 528063.375\n",
      "\n",
      "Эпоха: 877 Loss_train: 253176.6452174831, 0:00:00.088587 сек\n",
      "Loss_val: 528183.0\n",
      "\n",
      "Эпоха: 878 Loss_train: 253159.63719383447, 0:00:00.136568 сек\n",
      "Loss_val: 527645.5625\n",
      "\n",
      "Эпоха: 879 Loss_train: 253155.02164273648, 0:00:00.095568 сек\n",
      "Loss_val: 527889.25\n",
      "\n",
      "Эпоха: 880 Loss_train: 255248.66870777027, 0:00:00.095533 сек\n",
      "Loss_val: 527709.625\n",
      "\n",
      "Эпоха: 881 Loss_train: 253142.97650971284, 0:00:00.092572 сек\n",
      "Loss_val: 527434.6875\n",
      "\n",
      "Эпоха: 882 Loss_train: 254680.3606418919, 0:00:00.095039 сек\n",
      "Loss_val: 527245.1875\n",
      "\n",
      "Эпоха: 883 Loss_train: 253372.12705869932, 0:00:00.086606 сек\n",
      "Loss_val: 527100.8125\n",
      "\n",
      "Эпоха: 884 Loss_train: 252928.52797719595, 0:00:00.085542 сек\n",
      "Loss_val: 526974.8125\n",
      "\n",
      "Эпоха: 885 Loss_train: 253667.72270903716, 0:00:00.092017 сек\n",
      "Loss_val: 526801.9375\n",
      "\n",
      "Эпоха: 886 Loss_train: 253116.64949324325, 0:00:00.108789 сек\n",
      "Loss_val: 526446.4375\n",
      "\n",
      "Эпоха: 887 Loss_train: 253269.4026076858, 0:00:00.101179 сек\n",
      "Loss_val: 526525.625\n",
      "\n",
      "Эпоха: 888 Loss_train: 253185.62494721284, 0:00:00.101151 сек\n",
      "Loss_val: 526381.0\n",
      "\n",
      "Эпоха: 889 Loss_train: 253130.62114653716, 0:00:00.084200 сек\n",
      "Loss_val: 526427.5625\n",
      "\n",
      "Эпоха: 890 Loss_train: 252923.60298775337, 0:00:00.102064 сек\n",
      "Loss_val: 526002.4375\n",
      "\n",
      "Эпоха: 891 Loss_train: 253031.5735325169, 0:00:00.105099 сек\n",
      "Loss_val: 526191.4375\n",
      "\n",
      "Эпоха: 892 Loss_train: 253440.42657305743, 0:00:00.114863 сек\n",
      "Loss_val: 525883.3125\n",
      "\n",
      "Эпоха: 893 Loss_train: 252821.9055637669, 0:00:00.101099 сек\n",
      "Loss_val: 525665.1875\n",
      "\n",
      "Эпоха: 894 Loss_train: 253606.16902449325, 0:00:00.098669 сек\n",
      "Loss_val: 525618.5625\n",
      "\n",
      "Эпоха: 895 Loss_train: 253471.84462098818, 0:00:00.093190 сек\n",
      "Loss_val: 525379.6875\n",
      "\n",
      "Эпоха: 896 Loss_train: 253130.6760451858, 0:00:00.091638 сек\n",
      "Loss_val: 525023.8125\n",
      "\n",
      "Эпоха: 897 Loss_train: 253048.61164484796, 0:00:00.095154 сек\n",
      "Loss_val: 525331.25\n",
      "\n",
      "Эпоха: 898 Loss_train: 253196.82759712837, 0:00:00.102559 сек\n",
      "Loss_val: 524936.875\n",
      "\n",
      "Эпоха: 899 Loss_train: 253108.40493032095, 0:00:00.094555 сек\n",
      "Loss_val: 525002.8125\n",
      "\n",
      "Эпоха: 900 Loss_train: 252559.90564294765, 0:00:00.089230 сек\n",
      "Loss_val: 524815.125\n",
      "\n",
      "Эпоха: 901 Loss_train: 252784.1681007179, 0:00:00.114739 сек\n",
      "Loss_val: 524998.6875\n",
      "\n",
      "Эпоха: 902 Loss_train: 252690.02671030405, 0:00:00.114128 сек\n",
      "Loss_val: 524699.9375\n",
      "\n",
      "Эпоха: 903 Loss_train: 252809.1154983108, 0:00:00.106674 сек\n",
      "Loss_val: 524727.0\n",
      "\n",
      "Эпоха: 904 Loss_train: 253125.7854465794, 0:00:00.094520 сек\n",
      "Loss_val: 524407.0\n",
      "\n",
      "Эпоха: 905 Loss_train: 252989.18090160473, 0:00:00.095043 сек\n",
      "Loss_val: 524308.0\n",
      "\n",
      "Эпоха: 906 Loss_train: 252934.27940244932, 0:00:00.090150 сек\n",
      "Loss_val: 524342.0625\n",
      "\n",
      "Эпоха: 907 Loss_train: 252602.1773120777, 0:00:00.096101 сек\n",
      "Loss_val: 524041.03125\n",
      "\n",
      "Эпоха: 908 Loss_train: 252555.6454814189, 0:00:00.086085 сек\n",
      "Loss_val: 523888.09375\n",
      "\n",
      "Эпоха: 909 Loss_train: 253033.64769847973, 0:00:00.095178 сек\n",
      "Loss_val: 523950.21875\n",
      "\n",
      "Эпоха: 910 Loss_train: 252761.77269847973, 0:00:00.112354 сек\n",
      "Loss_val: 523758.65625\n",
      "\n",
      "Эпоха: 911 Loss_train: 252695.4437552787, 0:00:00.092038 сек\n",
      "Loss_val: 523572.125\n",
      "\n",
      "Эпоха: 912 Loss_train: 252527.3037109375, 0:00:00.094036 сек\n",
      "Loss_val: 523442.78125\n",
      "\n",
      "Эпоха: 913 Loss_train: 252548.56107474663, 0:00:00.075038 сек\n",
      "Loss_val: 523295.09375\n",
      "\n",
      "Эпоха: 914 Loss_train: 252559.28367820947, 0:00:00.087733 сек\n",
      "Loss_val: 523347.71875\n",
      "\n",
      "Эпоха: 915 Loss_train: 252997.49662162163, 0:00:00.097473 сек\n",
      "Loss_val: 523163.71875\n",
      "\n",
      "Эпоха: 916 Loss_train: 252291.86291173985, 0:00:00.089909 сек\n",
      "Loss_val: 523089.78125\n",
      "\n",
      "Эпоха: 917 Loss_train: 253227.9206081081, 0:00:00.122527 сек\n",
      "Loss_val: 522854.625\n",
      "\n",
      "Эпоха: 918 Loss_train: 252581.67762880068, 0:00:00.105123 сек\n",
      "Loss_val: 522687.96875\n",
      "\n",
      "Эпоха: 919 Loss_train: 253360.7347972973, 0:00:00.101036 сек\n",
      "Loss_val: 522478.53125\n",
      "\n",
      "Эпоха: 920 Loss_train: 252922.7222339527, 0:00:00.105599 сек\n",
      "Loss_val: 522603.875\n",
      "\n",
      "Эпоха: 921 Loss_train: 252306.9008129223, 0:00:00.117528 сек\n",
      "Loss_val: 522253.625\n",
      "\n",
      "Эпоха: 922 Loss_train: 252497.1371410473, 0:00:00.090073 сек\n",
      "Loss_val: 522270.21875\n",
      "\n",
      "Эпоха: 923 Loss_train: 252318.25675675675, 0:00:00.117115 сек\n",
      "Loss_val: 522084.5\n",
      "\n",
      "Эпоха: 924 Loss_train: 252345.0132495777, 0:00:00.104289 сек\n",
      "Loss_val: 522140.15625\n",
      "\n",
      "Эпоха: 925 Loss_train: 253391.53547297296, 0:00:00.106129 сек\n",
      "Loss_val: 522206.21875\n",
      "\n",
      "Эпоха: 926 Loss_train: 252239.24012880068, 0:00:00.104102 сек\n",
      "Loss_val: 521972.84375\n",
      "\n",
      "Эпоха: 927 Loss_train: 252822.8896748311, 0:00:00.088058 сек\n",
      "Loss_val: 521653.84375\n",
      "\n",
      "Эпоха: 928 Loss_train: 252564.99820523648, 0:00:00.089597 сек\n",
      "Loss_val: 521739.90625\n",
      "\n",
      "Эпоха: 929 Loss_train: 252324.32654138515, 0:00:00.092255 сек\n",
      "Loss_val: 521620.0\n",
      "\n",
      "Эпоха: 930 Loss_train: 252627.638671875, 0:00:00.105527 сек\n",
      "Loss_val: 521459.125\n",
      "\n",
      "Эпоха: 931 Loss_train: 252261.9917652027, 0:00:00.113043 сек\n",
      "Loss_val: 521295.875\n",
      "\n",
      "Эпоха: 932 Loss_train: 252129.87162162163, 0:00:00.104649 сек\n",
      "Loss_val: 521118.96875\n",
      "\n",
      "Эпоха: 933 Loss_train: 252317.5287162162, 0:00:00.129046 сек\n",
      "Loss_val: 520903.84375\n",
      "\n",
      "Эпоха: 934 Loss_train: 252350.96267947636, 0:00:00.098344 сек\n",
      "Loss_val: 520854.875\n",
      "\n",
      "Эпоха: 935 Loss_train: 252107.16395692568, 0:00:00.091646 сек\n",
      "Loss_val: 520853.40625\n",
      "\n",
      "Эпоха: 936 Loss_train: 252276.0199535473, 0:00:00.088451 сек\n",
      "Loss_val: 520791.84375\n",
      "\n",
      "Эпоха: 937 Loss_train: 251939.94869087837, 0:00:00.114114 сек\n",
      "Loss_val: 520765.34375\n",
      "\n",
      "Эпоха: 938 Loss_train: 252126.3840793919, 0:00:00.092979 сек\n",
      "Loss_val: 520360.125\n",
      "\n",
      "Эпоха: 939 Loss_train: 252538.11908783784, 0:00:00.088180 сек\n",
      "Loss_val: 520482.40625\n",
      "\n",
      "Эпоха: 940 Loss_train: 255321.97033361485, 0:00:00.086259 сек\n",
      "Loss_val: 520464.46875\n",
      "\n",
      "Эпоха: 941 Loss_train: 251836.28468116553, 0:00:00.097621 сек\n",
      "Loss_val: 520078.65625\n",
      "\n",
      "Эпоха: 942 Loss_train: 252063.25805004223, 0:00:00.095142 сек\n",
      "Loss_val: 519972.65625\n",
      "\n",
      "Эпоха: 943 Loss_train: 252157.03966955235, 0:00:00.100543 сек\n",
      "Loss_val: 519912.09375\n",
      "\n",
      "Эпоха: 944 Loss_train: 252104.3743137669, 0:00:00.085534 сек\n",
      "Loss_val: 519513.34375\n",
      "\n",
      "Эпоха: 945 Loss_train: 252604.59097867398, 0:00:00.084052 сек\n",
      "Loss_val: 519696.84375\n",
      "\n",
      "Эпоха: 946 Loss_train: 251911.97677364864, 0:00:00.091533 сек\n",
      "Loss_val: 519500.78125\n",
      "\n",
      "Эпоха: 947 Loss_train: 251768.35847761825, 0:00:00.093003 сек\n",
      "Loss_val: 519421.0\n",
      "\n",
      "Эпоха: 948 Loss_train: 252470.31115392735, 0:00:00.107562 сек\n",
      "Loss_val: 519609.90625\n",
      "\n",
      "Эпоха: 949 Loss_train: 251952.89405616553, 0:00:00.085114 сек\n",
      "Loss_val: 519099.375\n",
      "\n",
      "Эпоха: 950 Loss_train: 251813.90440244932, 0:00:00.102595 сек\n",
      "Loss_val: 519116.53125\n",
      "\n",
      "Эпоха: 951 Loss_train: 251825.23442778716, 0:00:00.090543 сек\n",
      "Loss_val: 518804.125\n",
      "\n",
      "Эпоха: 952 Loss_train: 252799.87267736485, 0:00:00.096054 сек\n",
      "Loss_val: 519043.5\n",
      "\n",
      "Эпоха: 953 Loss_train: 251891.79215054898, 0:00:00.088544 сек\n",
      "Loss_val: 518625.5\n",
      "\n",
      "Эпоха: 954 Loss_train: 252000.57136824325, 0:00:00.095328 сек\n",
      "Loss_val: 518703.28125\n",
      "\n",
      "Эпоха: 955 Loss_train: 252234.17461993243, 0:00:00.091519 сек\n",
      "Loss_val: 518665.5\n",
      "\n",
      "Эпоха: 956 Loss_train: 252116.13386824325, 0:00:00.087045 сек\n",
      "Loss_val: 518338.21875\n",
      "\n",
      "Эпоха: 957 Loss_train: 251848.75, 0:00:00.093735 сек\n",
      "Loss_val: 518342.125\n",
      "\n",
      "Эпоха: 958 Loss_train: 251994.61111697636, 0:00:00.081533 сек\n",
      "Loss_val: 518444.78125\n",
      "\n",
      "Эпоха: 959 Loss_train: 252000.7009079392, 0:00:00.081146 сек\n",
      "Loss_val: 517964.875\n",
      "\n",
      "Эпоха: 960 Loss_train: 251723.82020692568, 0:00:00.102913 сек\n",
      "Loss_val: 517964.125\n",
      "\n",
      "Эпоха: 961 Loss_train: 251561.92768158784, 0:00:00.095578 сек\n",
      "Loss_val: 517787.59375\n",
      "\n",
      "Эпоха: 962 Loss_train: 251705.8530933277, 0:00:00.091635 сек\n",
      "Loss_val: 517806.40625\n",
      "\n",
      "Эпоха: 963 Loss_train: 252210.8754222973, 0:00:00.102590 сек\n",
      "Loss_val: 517672.90625\n",
      "\n",
      "Эпоха: 964 Loss_train: 251452.4567673142, 0:00:00.089038 сек\n",
      "Loss_val: 517649.40625\n",
      "\n",
      "Эпоха: 965 Loss_train: 251537.1654349662, 0:00:00.090551 сек\n",
      "Loss_val: 517680.5\n",
      "\n",
      "Эпоха: 966 Loss_train: 251605.7717483108, 0:00:00.094682 сек\n",
      "Loss_val: 517445.65625\n",
      "\n",
      "Эпоха: 967 Loss_train: 252084.03832347973, 0:00:00.081565 сек\n",
      "Loss_val: 517590.40625\n",
      "\n",
      "Эпоха: 968 Loss_train: 251561.1804001267, 0:00:00.082582 сек\n",
      "Loss_val: 517336.59375\n",
      "\n",
      "Эпоха: 969 Loss_train: 251767.44132706925, 0:00:00.092756 сек\n",
      "Loss_val: 517189.90625\n",
      "\n",
      "Эпоха: 970 Loss_train: 251578.9657939189, 0:00:00.083085 сек\n",
      "Loss_val: 517202.875\n",
      "\n",
      "Эпоха: 971 Loss_train: 251653.12035472973, 0:00:00.085033 сек\n",
      "Loss_val: 516892.78125\n",
      "\n",
      "Эпоха: 972 Loss_train: 251654.8510082348, 0:00:00.101515 сек\n",
      "Loss_val: 516910.375\n",
      "\n",
      "Эпоха: 973 Loss_train: 251339.279296875, 0:00:00.093550 сек\n",
      "Loss_val: 516658.46875\n",
      "\n",
      "Эпоха: 974 Loss_train: 251294.42960831925, 0:00:00.102883 сек\n",
      "Loss_val: 516475.15625\n",
      "\n",
      "Эпоха: 975 Loss_train: 251289.89975717905, 0:00:00.098688 сек\n",
      "Loss_val: 516359.375\n",
      "\n",
      "Эпоха: 976 Loss_train: 251345.17166385136, 0:00:00.097000 сек\n",
      "Loss_val: 516473.84375\n",
      "\n",
      "Эпоха: 977 Loss_train: 252008.7100929054, 0:00:00.090167 сек\n",
      "Loss_val: 516224.25\n",
      "\n",
      "Эпоха: 978 Loss_train: 251745.74202913852, 0:00:00.097591 сек\n",
      "Loss_val: 516345.46875\n",
      "\n",
      "Эпоха: 979 Loss_train: 251836.587890625, 0:00:00.087687 сек\n",
      "Loss_val: 516240.90625\n",
      "\n",
      "Эпоха: 980 Loss_train: 251857.17657305743, 0:00:00.088133 сек\n",
      "Loss_val: 516152.875\n",
      "\n",
      "Эпоха: 981 Loss_train: 251514.8268581081, 0:00:00.098729 сек\n",
      "Loss_val: 515914.90625\n",
      "\n",
      "Эпоха: 982 Loss_train: 251502.40883657095, 0:00:00.099635 сек\n",
      "Loss_val: 515977.84375\n",
      "\n",
      "Эпоха: 983 Loss_train: 251320.39527027027, 0:00:00.091202 сек\n",
      "Loss_val: 515969.40625\n",
      "\n",
      "Эпоха: 984 Loss_train: 252047.28647592905, 0:00:00.090570 сек\n",
      "Loss_val: 515719.59375\n",
      "\n",
      "Эпоха: 985 Loss_train: 253692.91047297296, 0:00:00.092636 сек\n",
      "Loss_val: 515793.96875\n",
      "\n",
      "Эпоха: 986 Loss_train: 251344.5092113598, 0:00:00.088611 сек\n",
      "Loss_val: 515336.375\n",
      "\n",
      "Эпоха: 987 Loss_train: 251603.8733108108, 0:00:00.100559 сек\n",
      "Loss_val: 515437.59375\n",
      "\n",
      "Эпоха: 988 Loss_train: 251392.23643369932, 0:00:00.095599 сек\n",
      "Loss_val: 515381.46875\n",
      "\n",
      "Эпоха: 989 Loss_train: 251302.16876055743, 0:00:00.090000 сек\n",
      "Loss_val: 515098.5\n",
      "\n",
      "Эпоха: 990 Loss_train: 252382.0199535473, 0:00:00.094408 сек\n",
      "Loss_val: 515332.5\n",
      "\n",
      "Эпоха: 991 Loss_train: 253913.08960620777, 0:00:00.100606 сек\n",
      "Loss_val: 515326.21875\n",
      "\n",
      "Эпоха: 992 Loss_train: 250983.0701805321, 0:00:00.086150 сек\n",
      "Loss_val: 515137.90625\n",
      "\n",
      "Эпоха: 993 Loss_train: 251664.55711570947, 0:00:00.091153 сек\n",
      "Loss_val: 515141.40625\n",
      "\n",
      "Эпоха: 994 Loss_train: 251494.00992398648, 0:00:00.078591 сек\n",
      "Loss_val: 514743.25\n",
      "\n",
      "Эпоха: 995 Loss_train: 251142.05252322636, 0:00:00.089520 сек\n",
      "Loss_val: 514642.96875\n",
      "\n",
      "Эпоха: 996 Loss_train: 250989.0101879223, 0:00:00.086018 сек\n",
      "Loss_val: 514361.0\n",
      "\n",
      "Эпоха: 997 Loss_train: 251037.66052576015, 0:00:00.079078 сек\n",
      "Loss_val: 514352.125\n",
      "\n",
      "Эпоха: 998 Loss_train: 250807.4301097973, 0:00:00.088508 сек\n",
      "Loss_val: 514116.84375\n",
      "\n",
      "Эпоха: 999 Loss_train: 250965.2509501689, 0:00:00.086073 сек\n",
      "Loss_val: 514088.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'dataset': MyDataset,\n",
    "    'net': net,\n",
    "    'epoch_amount': 1000,\n",
    "    'learning_rate': 0.00001,\n",
    "    'early_stopping': 25,\n",
    "    'loss_f': nn.MSELoss(),\n",
    "    'optim': torch.optim.Adam#torch.optim.SGD,\n",
    "}\n",
    "\n",
    "clf = Trainer(**params)\n",
    "clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5179dbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(Loss_train, Loss_val):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(range(len(Loss_train)), Loss_train, color='orange', label='train', linestyle='--')\n",
    "    plt.plot(range(len(Loss_val)), Loss_val, color='blue', marker='o', label='val')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(clf.train_loss, clf.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "640bc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure model is in eval mode\n",
    "clf.best_model.eval()\n",
    "\n",
    "# Prepare input\n",
    "X_val_np = X_val.to_numpy().astype(np.float32)\n",
    "X_val_tensor = torch.from_numpy(X_val_np).to(clf.device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    y_pred = clf.predict(X_val_tensor).view(-1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e4d1de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Price_euros\n",
      "0      0   444.872345\n",
      "1      1  3071.000244\n",
      "2      2   908.386292\n",
      "3      3  1248.110229\n",
      "4      4   497.557648\n",
      "     index  Price_euros\n",
      "190    190   315.126160\n",
      "191    191   462.424072\n",
      "192    192   913.257935\n",
      "193    193  1690.527710\n",
      "194    194   864.930664\n",
      "✅ Submission file 'Submission.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "expected_length = 195\n",
    "actual_length = len(y_pred)\n",
    "\n",
    "if actual_length != expected_length:\n",
    "    raise ValueError(f\"Expected {expected_length} predictions, but got {actual_length}.\")\n",
    "\n",
    "index_values = np.arange(expected_length)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': index_values,\n",
    "    'Price_euros': y_pred\n",
    "})\n",
    "\n",
    "print(submission_df.head())\n",
    "print(submission_df.tail())\n",
    "\n",
    "submission_df.to_csv('Submission.csv', index=False)\n",
    "print(\"✅ Submission file 'Submission.csv' has been created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neural_spr_2025_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
